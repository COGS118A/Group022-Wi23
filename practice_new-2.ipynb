{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Project Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Neil Bajaj\n",
    "- Pratheek Sankeshi\n",
    "- Shenova Davis\n",
    "- Ria Singh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "The goal of this project is to predict whether to approve a credit card for an applicant based on a variety of factors that were asked on their application. We will predict using a machine learning algorithm. The data we will be using is from the Kaggle dataset: A Credit Card Dataset for Machine Learning. The link is https://www.kaggle.com/datasets/caesarmario/application-data/. We will be using factors such as Total Income, Education Type, Applicant Age, etc. More will be described in the data section. Furthermore, we will drop unnecessary columns like Owned Phone, Owned Email, etc while also replacing null values to perform EDA. We will then run various supervised machine learning algorithms to create models to predict the data and use the best one. The performance will be measured on how accurately we predict the data against the status column of the data which is whether the application was approved or not. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "As we get deeper into the age of Big Data, we see that, where possible, we are attempting to move from human evaluation to machine learning prediction. There has also been an uptick in the number of credit card defaulters. Using this logic, credit card companies need a new way to decide whether or not to approve credit cards based on prior history. \n",
    "\n",
    "A previous study done by Dr. Hemkiran[1] evaluated whether applicants should be approved for a credit card by using a Logistic Regression with and without a grid search technique. They found that using a grid search technique improved the competency of their model. Additionally, they also used an Artificial Neural Network (ANN) and found it to be better than the linear regression model. Another study by Dr. Kibria[2] aimed to create a deep learning model to aid credit card approval decision-making. They also used a logistic regression model and a support vector machine (SVM) model to compare their results. They found that the deep learning model was better than the logistic regression and SVM models. However, the ANN model and deep learning model are more computationally expensive and time-consuming. \n",
    "\n",
    "We are attempting to create a predictive machine learning model that models whether or not the application has the right credentials to have their credit card application approved using the following variables: Applicant Gender, Owned property, Total Children, Owned Car, Total Income, Housing Type, Total Family Members, Applicant Age, Education Type, and Family Status. This is to avoid future credit card defaulters. We will use models such as Naive Bayes Classifier, Linear SVM, and Linear Regression and use them to compare our model’s performance as well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "We are building this model to measure if there are any discrepancies when approving credit card applications. We additionally want to check if we could use big data and machine learning to build a model that predicts if a credit card is approved or not. This would take out human bias from the equation and make sure every application is fairly reviewed. Creating a machine learning model will attempt to eliminate the human bias towards race, class, gender, etc - an issue that plagues the financial ratings of individuals. Additionally, a machine-learning system can significantly reduce the human power and costs of a credit card company, increasing revenue. If our model can accurately predict whether an application can be accepted, we could conclude our hypothesis. This problem is quantifiable since we are trying to model a binary predictor. This problem is measurable because we would be using the metric of accuracy to validate the performance of our model. Lastly, our model is replicable because we could run it on different datasets and check its accuracy on each dataset. The model we will create will be composed of supervised machine-learning algorithms and techniques such as logistic regression, linear SVM, K fold validation, etc. We will train our model on previously collected data from credit card companies to understand what attributes make an individual more or less likely to get approved for a credit card. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "\n",
    "- Link for our data: https://www.kaggle.com/datasets/caesarmario/application-data\n",
    "\n",
    "- This data set has about 25,100 observations with 21 variables. \n",
    "\n",
    "- The variables that will be used are Applicant Gender, Owned property, Total Children, Owned Car, Total Income, Housing Type, Total Family Members, Applicant Age, Education Type, and Family Status. \n",
    "\n",
    "- The dataset we are using has already been cleaned to drop any data points with null values and yet the dataset remains robust so we will not be addressing it any further.\n",
    "\n",
    "- We will additionally drop all the features that we will not be using to further declutter the dataset.\n",
    "\n",
    "- In addition to cleaning we one hot encoded all other categorical data which wasn't already binary which is displayed below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns    \n",
    "from matplotlib.pyplot import figure\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "#models importing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "\n",
    "#linear svm stuff\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Applicant_ID</th>\n",
       "      <th>Applicant_Gender</th>\n",
       "      <th>Owned_Car</th>\n",
       "      <th>Owned_Realty</th>\n",
       "      <th>Total_Children</th>\n",
       "      <th>Total_Income</th>\n",
       "      <th>Income_Type</th>\n",
       "      <th>Education_Type</th>\n",
       "      <th>Family_Status</th>\n",
       "      <th>Housing_Type</th>\n",
       "      <th>...</th>\n",
       "      <th>Owned_Work_Phone</th>\n",
       "      <th>Owned_Phone</th>\n",
       "      <th>Owned_Email</th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Total_Family_Members</th>\n",
       "      <th>Applicant_Age</th>\n",
       "      <th>Years_of_Working</th>\n",
       "      <th>Total_Bad_Debt</th>\n",
       "      <th>Total_Good_Debt</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5008806</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>112500</td>\n",
       "      <td>Working                                       ...</td>\n",
       "      <td>Secondary / secondary special                 ...</td>\n",
       "      <td>Married                                       ...</td>\n",
       "      <td>House / apartment                             ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Security staff                                ...</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5008808</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>270000</td>\n",
       "      <td>Commercial associate                          ...</td>\n",
       "      <td>Secondary / secondary special                 ...</td>\n",
       "      <td>Single / not married                          ...</td>\n",
       "      <td>House / apartment                             ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sales staff                                   ...</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5008809</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>270000</td>\n",
       "      <td>Commercial associate                          ...</td>\n",
       "      <td>Secondary / secondary special                 ...</td>\n",
       "      <td>Single / not married                          ...</td>\n",
       "      <td>House / apartment                             ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sales staff                                   ...</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5008810</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>270000</td>\n",
       "      <td>Commercial associate                          ...</td>\n",
       "      <td>Secondary / secondary special                 ...</td>\n",
       "      <td>Single / not married                          ...</td>\n",
       "      <td>House / apartment                             ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sales staff                                   ...</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5008811</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>270000</td>\n",
       "      <td>Commercial associate                          ...</td>\n",
       "      <td>Secondary / secondary special                 ...</td>\n",
       "      <td>Single / not married                          ...</td>\n",
       "      <td>House / apartment                             ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sales staff                                   ...</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Applicant_ID Applicant_Gender  Owned_Car  Owned_Realty  Total_Children  \\\n",
       "0       5008806          M                1             1               0   \n",
       "1       5008808          F                0             1               0   \n",
       "2       5008809          F                0             1               0   \n",
       "3       5008810          F                0             1               0   \n",
       "4       5008811          F                0             1               0   \n",
       "\n",
       "   Total_Income                                        Income_Type  \\\n",
       "0        112500  Working                                       ...   \n",
       "1        270000  Commercial associate                          ...   \n",
       "2        270000  Commercial associate                          ...   \n",
       "3        270000  Commercial associate                          ...   \n",
       "4        270000  Commercial associate                          ...   \n",
       "\n",
       "                                      Education_Type  \\\n",
       "0  Secondary / secondary special                 ...   \n",
       "1  Secondary / secondary special                 ...   \n",
       "2  Secondary / secondary special                 ...   \n",
       "3  Secondary / secondary special                 ...   \n",
       "4  Secondary / secondary special                 ...   \n",
       "\n",
       "                                       Family_Status  \\\n",
       "0  Married                                       ...   \n",
       "1  Single / not married                          ...   \n",
       "2  Single / not married                          ...   \n",
       "3  Single / not married                          ...   \n",
       "4  Single / not married                          ...   \n",
       "\n",
       "                                        Housing_Type  ...  Owned_Work_Phone  \\\n",
       "0  House / apartment                             ...  ...                 0   \n",
       "1  House / apartment                             ...  ...                 0   \n",
       "2  House / apartment                             ...  ...                 0   \n",
       "3  House / apartment                             ...  ...                 0   \n",
       "4  House / apartment                             ...  ...                 0   \n",
       "\n",
       "   Owned_Phone  Owned_Email  \\\n",
       "0            0            0   \n",
       "1            1            1   \n",
       "2            1            1   \n",
       "3            1            1   \n",
       "4            1            1   \n",
       "\n",
       "                                           Job_Title Total_Family_Members  \\\n",
       "0  Security staff                                ...                    2   \n",
       "1  Sales staff                                   ...                    1   \n",
       "2  Sales staff                                   ...                    1   \n",
       "3  Sales staff                                   ...                    1   \n",
       "4  Sales staff                                   ...                    1   \n",
       "\n",
       "   Applicant_Age  Years_of_Working  Total_Bad_Debt  Total_Good_Debt  Status  \n",
       "0             59                 4               0               30       1  \n",
       "1             53                 9               0                5       1  \n",
       "2             53                 9               0                5       1  \n",
       "3             53                 9               0               27       1  \n",
       "4             53                 9               0               39       1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit  = pd.read_csv('Application_Data.csv')\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004815345431391276\n"
     ]
    }
   ],
   "source": [
    "status = credit['Status']\n",
    "status_0_len = len(status[status==0])\n",
    "status_1_len = len(status[status==1])\n",
    "print(status_0_len/len(status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMODELS TO USE \\n- LOGISTIC REGRESSION / CLASSIFIER \\n- ADA BOOST \\n- GRADIENT BOOST \\n- RANDOM FOREST \\n- SVM ?\\n'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#note: we noticed here that the datset mainly leading to approving the credit card, we cannot use a regualr model\n",
    "# because the best one would be just approving the credit card application every time. \n",
    "\n",
    "#we need to find a model that takes care of imbalanced data and looking fo that we found pyCaret. \n",
    "#we could also use scikit learn for this. \n",
    "\n",
    "#SMOTE\n",
    "#ADASYN\n",
    "\n",
    "'''\n",
    "MODELS TO USE \n",
    "- LOGISTIC REGRESSION / CLASSIFIER \n",
    "- ADA BOOST \n",
    "- GRADIENT BOOST \n",
    "- RANDOM FOREST \n",
    "- SVM ?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Applicant_ID             int64\n",
       "Applicant_Gender        object\n",
       "Owned_Car                int64\n",
       "Owned_Realty             int64\n",
       "Total_Children           int64\n",
       "Total_Income             int64\n",
       "Income_Type             object\n",
       "Education_Type          object\n",
       "Family_Status           object\n",
       "Housing_Type            object\n",
       "Owned_Mobile_Phone       int64\n",
       "Owned_Work_Phone         int64\n",
       "Owned_Phone              int64\n",
       "Owned_Email              int64\n",
       "Job_Title               object\n",
       "Total_Family_Members     int64\n",
       "Applicant_Age            int64\n",
       "Years_of_Working         int64\n",
       "Total_Bad_Debt           int64\n",
       "Total_Good_Debt          int64\n",
       "Status                   int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we will only keep columns that we need. These columns are covered under Data along with \n",
    "#status which says whether or not it was approved\n",
    "credit = credit[[\"Applicant_Gender\", \"Applicant_Age\",\"Owned_Realty\",\n",
    "                 \"Total_Children\", \"Owned_Car\", \"Total_Income\", \"Housing_Type\",\n",
    "                 \"Total_Family_Members\", \"Education_Type\", \"Family_Status\", 'Status', 'Total_Bad_Debt', 'Total_Good_Debt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Applicant_Gender        object\n",
       "Applicant_Age            int64\n",
       "Owned_Realty             int64\n",
       "Total_Children           int64\n",
       "Owned_Car                int64\n",
       "Total_Income             int64\n",
       "Housing_Type            object\n",
       "Total_Family_Members     int64\n",
       "Education_Type          object\n",
       "Family_Status           object\n",
       "Status                   int64\n",
       "Total_Bad_Debt           int64\n",
       "Total_Good_Debt          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split since its a large dataset idk about this one shud explain this better than we did\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(credit.drop('Status', axis=1), #\n",
    "                                                    credit['Status'], #Y variable target\n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=123)\n",
    "categorical_features = [\"Applicant_Gender\", \"Housing_Type\", \"Education_Type\", \"Family_Status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THE CODE IN THE CELL BELOW USES SMOTE TO FIX DATA IMBALANCES \n",
    "# OUR DATAPOINTS WENT FROM 20K IN THE TRAINSET TO 40K IN THE TRAINSET \n",
    "#we have shifted our status to the y_test and y_train because that is what we are going to compare with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Applicant_Gender        object\n",
       "Applicant_Age            int64\n",
       "Owned_Realty             int64\n",
       "Total_Children           int64\n",
       "Owned_Car                int64\n",
       "Total_Income             int64\n",
       "Housing_Type            object\n",
       "Total_Family_Members     int64\n",
       "Education_Type          object\n",
       "Family_Status           object\n",
       "Total_Bad_Debt           int64\n",
       "Total_Good_Debt          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtypes #showing what the different columns are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "\n",
    "# Define numerical and categorical column selectors\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "# Get numerical and categorical column names\n",
    "numerical_columns = numerical_columns_selector(X_train)\n",
    "categorical_columns = categorical_columns_selector(X_train)\n",
    "\n",
    "# Define one-hot encoder and scaler\n",
    "numerical_transformer = StandardScaler()\n",
    "#categorical_transformer = OneHotEncoder(drop='if_binary')  # drop original categorical features if binary (applicant_gender)\n",
    "\n",
    "categorical_transformer = OneHotEncoder(drop='first')  # drop original categorical features, could combine with if binary\n",
    "#but unnecessary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#creating a preprocessor to add to the pipeline maybe combine this cell into one?\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[ \n",
    "        ('num', numerical_transformer, numerical_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns)\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Applicant_Gender</th>\n",
       "      <th>Applicant_Age</th>\n",
       "      <th>Owned_Realty</th>\n",
       "      <th>Total_Children</th>\n",
       "      <th>Owned_Car</th>\n",
       "      <th>Total_Income</th>\n",
       "      <th>Housing_Type</th>\n",
       "      <th>Total_Family_Members</th>\n",
       "      <th>Education_Type</th>\n",
       "      <th>Family_Status</th>\n",
       "      <th>Total_Bad_Debt</th>\n",
       "      <th>Total_Good_Debt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21319</th>\n",
       "      <td>M</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112500</td>\n",
       "      <td>House / apartment                             ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Secondary / secondary special                 ...</td>\n",
       "      <td>Married                                       ...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21048</th>\n",
       "      <td>M</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>135000</td>\n",
       "      <td>House / apartment                             ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Secondary / secondary special                 ...</td>\n",
       "      <td>Married                                       ...</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15222</th>\n",
       "      <td>M</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>157500</td>\n",
       "      <td>House / apartment                             ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Secondary / secondary special                 ...</td>\n",
       "      <td>Married                                       ...</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8330</th>\n",
       "      <td>F</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135000</td>\n",
       "      <td>With parents                                  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary / secondary special                 ...</td>\n",
       "      <td>Single / not married                          ...</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10931</th>\n",
       "      <td>M</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180000</td>\n",
       "      <td>House / apartment                             ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Secondary / secondary special                 ...</td>\n",
       "      <td>Married                                       ...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Applicant_Gender  Applicant_Age  Owned_Realty  Total_Children  \\\n",
       "21319          M                   52             0               0   \n",
       "21048          M                   35             0               1   \n",
       "15222          M                   31             1               1   \n",
       "8330           F                   55             0               0   \n",
       "10931          M                   38             0               0   \n",
       "\n",
       "       Owned_Car  Total_Income  \\\n",
       "21319          0        112500   \n",
       "21048          0        135000   \n",
       "15222          1        157500   \n",
       "8330           0        135000   \n",
       "10931          0        180000   \n",
       "\n",
       "                                            Housing_Type  \\\n",
       "21319  House / apartment                             ...   \n",
       "21048  House / apartment                             ...   \n",
       "15222  House / apartment                             ...   \n",
       "8330   With parents                                  ...   \n",
       "10931  House / apartment                             ...   \n",
       "\n",
       "       Total_Family_Members  \\\n",
       "21319                     2   \n",
       "21048                     3   \n",
       "15222                     3   \n",
       "8330                      1   \n",
       "10931                     2   \n",
       "\n",
       "                                          Education_Type  \\\n",
       "21319  Secondary / secondary special                 ...   \n",
       "21048  Secondary / secondary special                 ...   \n",
       "15222  Secondary / secondary special                 ...   \n",
       "8330   Secondary / secondary special                 ...   \n",
       "10931  Secondary / secondary special                 ...   \n",
       "\n",
       "                                           Family_Status  Total_Bad_Debt  \\\n",
       "21319  Married                                       ...               0   \n",
       "21048  Married                                       ...               0   \n",
       "15222  Married                                       ...               2   \n",
       "8330   Single / not married                          ...               0   \n",
       "10931  Married                                       ...               0   \n",
       "\n",
       "       Total_Good_Debt  \n",
       "21319               12  \n",
       "21048               17  \n",
       "15222               18  \n",
       "8330                40  \n",
       "10931               21  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns and apply preprocessor to remaining features\n",
    "#X_train_processed = preprocessor.fit_transform(X_train, y_train) \n",
    "X_train_processed = preprocessor.fit_transform(X_train) #no difference between putting a target or not\n",
    "'''\n",
    "The explanation from chatgpt:\n",
    "It means that the target variable y_train is not used in the preprocessing step, \n",
    "so passing it as an argument to fit_transform() or not passing it has no effect on the output \n",
    "of X_train_processed. The preprocessor only transforms the features in X_train according to \n",
    "the specifications in preprocessor, so the target variable is not needed. However, if you had \n",
    "a preprocessor that needed the target variable, then passing it as an argument would be necessary.\n",
    "\n",
    "'''\n",
    "#combines teh .fit and .transform step of the ColumnTransformer ^^\n",
    "\n",
    "#oversample using the smote technique which was recommended on piazza\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_train_processed, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40004, 22)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_res.shape #so we managed to drop the original columsn from the data because if we didnt drop the \n",
    "#first then it would be x26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_processed )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for the crossvalidation chain we have to convert the X_res which is a numpy drray back into a pandas dataframe\n",
    "# Convert X_res to a DataFrame\n",
    "#column_names = preprocessor.get_feature_names() #would have worked if standard scalar didnt have the get_feature_name method\n",
    "\n",
    "# Get column names for the DataFrame\n",
    "num_columns = list(numerical_columns)\n",
    "\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "#cat_columns = list(categorical_transformer.get_feature_names_out(categorical_columns))\n",
    "cat_columns = list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_columns))\n",
    "column_names = num_columns + cat_columns\n",
    "column_names = num_columns + cat_columns\n",
    "\n",
    "X_res_df = pd.DataFrame(X_res, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -0.797369\n",
       "1   -0.578088\n",
       "2   -0.358807\n",
       "3   -0.578088\n",
       "4   -0.139526\n",
       "Name: Total_Income, dtype: float64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_res_df['Total_Income'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    40004.000000\n",
       "mean        -0.041293\n",
       "std          1.006789\n",
       "min         -1.630637\n",
       "25%         -0.660573\n",
       "50%         -0.315613\n",
       "75%          0.299036\n",
       "max         13.455899\n",
       "Name: Total_Income, dtype: float64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_res_df['Total_Income'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2.010200e+04\n",
       "mean     1.943165e+05\n",
       "std      1.026106e+05\n",
       "min      2.700000e+04\n",
       "25%      1.350000e+05\n",
       "50%      1.800000e+05\n",
       "75%      2.250000e+05\n",
       "max      1.575000e+06\n",
       "Name: Total_Income, dtype: float64"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['Total_Income'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe reason why they look different and transformed is due to standard scaling \\nStandard scaling (also known as standardization) has several benefits:\\n\\nNormalization of data: Standard scaling transforms the data to have zero mean and unit variance, which helps in normalizing the data. This is particularly useful when the data has different units of measurement or scales.\\n\\nBetter performance of some machine learning algorithms: Some machine learning algorithms like K-nearest neighbors (KNN) and SVM (support vector machines) are sensitive to the scale of the input features. Standard scaling can improve the performance of these algorithms.\\n\\nEfficient optimization: Many optimization algorithms like gradient descent converge faster when the input features are on the same scale.\\n\\nInterpretation of coefficients: When performing linear regression or other models with coefficients, standard scaling ensures that the coefficients can be compared fairly, as they are on the same scale.\\n\\nOverall, standard scaling is a common preprocessing step that can improve the performance of many machine learning models and make the interpretation of results easier.\\n\\n\\nDue to the reasons above, I believe we should keep standard scaling\\n\\n'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "The reason why they look different and transformed is due to standard scaling \n",
    "Standard scaling (also known as standardization) has several benefits:\n",
    "\n",
    "Normalization of data: Standard scaling transforms the data to have zero mean and unit variance, which helps in normalizing the data. This is particularly useful when the data has different units of measurement or scales.\n",
    "\n",
    "Better performance of some machine learning algorithms: Some machine learning algorithms like K-nearest neighbors (KNN) and SVM (support vector machines) are sensitive to the scale of the input features. Standard scaling can improve the performance of these algorithms.\n",
    "\n",
    "Efficient optimization: Many optimization algorithms like gradient descent converge faster when the input features are on the same scale.\n",
    "\n",
    "Interpretation of coefficients: When performing linear regression or other models with coefficients, standard scaling ensures that the coefficients can be compared fairly, as they are on the same scale.\n",
    "\n",
    "Overall, standard scaling is a common preprocessing step that can improve the performance of many machine learning models and make the interpretation of results easier.\n",
    "\n",
    "\n",
    "Due to the reasons above, I believe we should keep standard scaling\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 25 folds for each of 3 candidates, totalling 75 fits\n",
      "{'mean_fit_time': array([0.35520584, 0.53480962, 0.86559887]), 'std_fit_time': array([0.0446156 , 0.02723175, 0.12175941]), 'mean_score_time': array([0.00578008, 0.00564754, 0.00538681]), 'std_score_time': array([0.00127699, 0.0005519 , 0.00184771]), 'param_C': masked_array(data=[0.01, 0.05, 10],\n",
      "             mask=[False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 0.01}, {'C': 0.05}, {'C': 10}], 'split0_test_score': array([0.98950131, 0.992001  , 0.99850019]), 'split1_test_score': array([0.99150106, 0.9936258 , 0.99825022]), 'split2_test_score': array([0.99350081, 0.99612548, 0.99937508]), 'split3_test_score': array([0.99262592, 0.99537558, 0.99900012]), 'split4_test_score': array([0.992625, 0.994   , 0.998625]), 'split5_test_score': array([0.99025122, 0.99400075, 0.99900012]), 'split6_test_score': array([0.99337583, 0.99425072, 0.9983752 ]), 'split7_test_score': array([0.99287589, 0.99575053, 0.99925009]), 'split8_test_score': array([0.99312586, 0.99425072, 0.99887514]), 'split9_test_score': array([0.99025 , 0.992875, 0.998   ]), 'split10_test_score': array([0.98937633, 0.99287589, 0.99875016]), 'split11_test_score': array([0.99212598, 0.99375078, 0.9983752 ]), 'split12_test_score': array([0.99325084, 0.99512561, 0.99937508]), 'split13_test_score': array([0.99375078, 0.99562555, 0.99875016]), 'split14_test_score': array([0.991125, 0.994875, 0.998875]), 'split15_test_score': array([0.99162605, 0.99400075, 0.99887514]), 'split16_test_score': array([0.992001  , 0.9936258 , 0.99875016]), 'split17_test_score': array([0.99175103, 0.99525059, 0.99875016]), 'split18_test_score': array([0.99237595, 0.99425072, 0.99862517]), 'split19_test_score': array([0.991875, 0.994   , 0.998625]), 'split20_test_score': array([0.99212598, 0.99450069, 0.99937508]), 'split21_test_score': array([0.99100112, 0.99312586, 0.99812523]), 'split22_test_score': array([0.99212598, 0.99425072, 0.99862517]), 'split23_test_score': array([0.99225097, 0.99462567, 0.99900012]), 'split24_test_score': array([0.992375, 0.9945  , 0.99875 ]), 'mean_test_score': array([0.9919508 , 0.99426557, 0.99875512]), 'std_test_score': array([0.00115589, 0.00095342, 0.00036137]), 'rank_test_score': array([3, 2, 1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# REPEATED K FOLD CROSS VALIDATION \n",
    "# 25 FOLDS FOR EACH OF THE 3 CANDIDATES - TOTALLING 75 FITS\n",
    "\n",
    "# Import here\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "params ={'C':[0.01, 0.05, 10]}\n",
    "rkf = RepeatedKFold(n_splits=5, n_repeats=5, random_state = 42)\n",
    "\n",
    "clf1 = GridSearchCV(LogisticRegression(), params, cv = rkf, n_jobs=-1, verbose = 3)\n",
    "\n",
    "\n",
    "#i think we have to fit something here\n",
    "#clf1.fit(X_train, y_train)\n",
    "clf1.fit(X_res_df, y_res)\n",
    "# View the Grid Search CV Results\n",
    "print((clf1.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res = X_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(clf1.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([0.10931808, 0.28320243, 0.29437852]), 'std_fit_time': array([0.01108922, 0.02303357, 0.03196705]), 'mean_score_time': array([0.00062051, 0.00084024, 0.00070312]), 'std_score_time': array([0.0005161 , 0.00036672, 0.00049214]), 'param_C': masked_array(data=[0.01, 0.5, 10],\n",
      "             mask=[False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 0.01}, {'C': 0.5}, {'C': 10}], 'split0_test_score': array([0.9806371 , 0.99250468, 0.99875078]), 'split1_test_score': array([0.98500937, 0.99250468, 0.99625234]), 'split2_test_score': array([0.9868832 , 0.99562773, 0.99875078]), 'split3_test_score': array([0.98625859, 0.99687695, 0.99875078]), 'split4_test_score': array([0.985625, 0.993125, 0.998125]), 'split5_test_score': array([0.981875, 0.99    , 0.995625]), 'split6_test_score': array([0.98125 , 0.991875, 0.99875 ]), 'split7_test_score': array([0.985   , 0.993125, 0.996875]), 'split8_test_score': array([0.98125 , 0.991875, 0.9975  ]), 'split9_test_score': array([0.98125 , 0.994375, 0.996875]), 'split10_test_score': array([0.985  , 0.99375, 0.9975 ]), 'split11_test_score': array([0.985   , 0.995625, 0.99875 ]), 'split12_test_score': array([0.991875, 0.99625 , 0.99875 ]), 'split13_test_score': array([1., 1., 1.]), 'split14_test_score': array([1., 1., 1.]), 'split15_test_score': array([1., 1., 1.]), 'split16_test_score': array([1., 1., 1.]), 'split17_test_score': array([1., 1., 1.]), 'split18_test_score': array([1., 1., 1.]), 'split19_test_score': array([1., 1., 1.]), 'split20_test_score': array([1., 1., 1.]), 'split21_test_score': array([1., 1., 1.]), 'split22_test_score': array([1., 1., 1.]), 'split23_test_score': array([1., 1., 1.]), 'split24_test_score': array([1., 1., 1.]), 'mean_test_score': array([0.99187653, 0.99670056, 0.99885019]), 'std_test_score': array([0.00810335, 0.00346145, 0.00134238]), 'rank_test_score': array([3, 2, 1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# NORMAL K FOLD CROSS VALIDATION SPLITS =25 \n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, GridSearchCV\n",
    "\n",
    "params = {'C': [0.01, 0.5, 10]}\n",
    "\n",
    "kf = KFold(n_splits = 25)\n",
    "clf2 = GridSearchCV(LogisticRegression(), params, cv = kf)\n",
    "\n",
    "clf2.fit(X_res, y_res)\n",
    "\n",
    "# View the Grid Search CV Results\n",
    "print(clf2.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9806371  0.98500937 0.9868832  0.98625859 0.985625   0.981875\n",
      " 0.98125    0.985      0.98125    0.98125    0.985      0.985\n",
      " 0.991875   1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# COMPARING THE TWO MODELS WE JUST RAN TO SEE WHICH MODEL RETURNS A BETTER ACCUARACY ON OUR DATASET\n",
    "\n",
    "clfi1Mat = np.zeros((25,3))\n",
    "clfi2Mat = np.zeros((25,3))\n",
    "i = 0\n",
    "for key in clf1.cv_results_.keys():\n",
    "    if 'split' in key:\n",
    "        clfi1Mat[i] = clf1.cv_results_[key]\n",
    "        clfi2Mat[i] = clf2.cv_results_[key]\n",
    "        i += 1\n",
    "#print(clfi1Mat.shape)\n",
    "#print(clfi2Mat)#so we go \n",
    "\n",
    "#next we have to get 9 different arrays for each c value\n",
    "clfi1MatC1 = clfi1Mat[:, 0] #go by column\n",
    "clfi2MatC1 = clfi2Mat[:, 0]\n",
    "print(clfi2MatC1)\n",
    "clfi1MatC2 = clfi1Mat[:, 1] #go by column\n",
    "clfi2MatC2 = clfi2Mat[:, 1]\n",
    "\n",
    "clfi1MatC3 = clfi1Mat[:, 2] #go by column\n",
    "clfi2MatC3 = clfi2Mat[:, 2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9941257342832146\n",
      "0.9960004999375078\n",
      "0.9993750781152356\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#ACCUARACY OF EACH COMBINATION OF OUR MODEL \n",
    "# MAXIMUM AXXURACY ACHIEVED HERE IS 55.9% WHICH ISNT GREAT AND IS WORSE THAN OUR BASELINE \n",
    "#HENCE WE NEED TO TRY OTHER MODELS\n",
    "\n",
    "print(max(clfi1MatC1))\n",
    "print(max(clfi1MatC2))\n",
    "print(max(clfi1MatC3))\n",
    "print(max(clfi2MatC1))\n",
    "print(max(clfi2MatC2))\n",
    "print(max(clfi2MatC3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT FOR OUR REPEATED K - FOLDS AND OUR K FOLD CROSS VALIDATION MODELS \n",
    "# WE CAN SEE THAT OUR REPEATED K FOLDS PERFORMS SIGNIFICANTLY BETTER \n",
    "\n",
    "#next we have to do 3 plots\n",
    "xVals = np.arange(25)\n",
    "print(xVals)\n",
    "plt.plot(xVals, clfi1MatC1, color = 'blue', label = \"5x5 C=0.01\")\n",
    "plt.plot(xVals, clfi2MatC1, color = 'orange', label = \"25 C=0.01\")\n",
    "plt.title(\"Repeated KFolds vs KFolds for C=0.01\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT FOR OUR REPEATED K - FOLDS AND OUR K FOLD CROSS VALIDATION MODELS \n",
    "# WE CAN SEE THAT OUR REPEATED K FOLDS PERFORMS SIGNIFICANTLY BETTER \n",
    "\n",
    "\n",
    "#next we have to do 3 plots\n",
    "xVals = np.arange(25)\n",
    "print(xVals)\n",
    "plt.plot(xVals, clfi1MatC2, color = 'blue', label = \"5x5 C=0.5\")\n",
    "plt.plot(xVals, clfi2MatC2, color = 'orange', label = \"25 C=0.5\")\n",
    "plt.title(\"Repeated KFolds vs KFolds for C=0.5\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT FOR OUR REPEATED K - FOLDS AND OUR K FOLD CROSS VALIDATION MODELS \n",
    "# WE CAN SEE THAT OUR REPEATED K FOLDS PERFORMS SIGNIFICANTLY BETTER \n",
    "\n",
    "\n",
    "#next we have to do 3 plots\n",
    "xVals = np.arange(25)\n",
    "print(xVals)\n",
    "plt.plot(xVals, clfi1MatC3, color = 'blue', label = \"5x5 C=10\")\n",
    "plt.plot(xVals, clfi2MatC3, color = 'orange', label = \"25 C=10\")\n",
    "plt.title(\"Repeated KFolds vs KFolds for C=10\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO NUMM VALUES IN OUR DATA - JUST A CHECK \n",
    "\n",
    "#next we check for nulls and nans and if there exists any we will count them\n",
    "#and decide to drop the row containing that datapooint\n",
    "\n",
    "credit.isnull().values.any()\n",
    "#so there isnt any nan values so we dont need to replace any values anywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next is making all columns which arent numerical numerical\n",
    "#first we will check which columns arent numerical\n",
    "credit.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT CODE - DROPPING COLUMNS WE DO NOT NEED (OF TYPE FLOAT AND DOESNT MAKE SENSE TO CONVERT USING ONE HOT ENCODING)\n",
    "\n",
    "X_test = X_test.drop(columns = ['Applicant_Gender', 'Housing_Type', 'Education_Type','Family_Status'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST A LOGISTIC CLASSIFIER - NO K FOLD OR REPEATED K FOLD USED HERE\n",
    "# ACCUARACY ACHIEVED WAS 58.27% WHICH IS BETTER BUT STILL NOT GOOD ENOUGH \n",
    "# L2 PENALTY USED \n",
    "\n",
    "\n",
    "#Just classifier no kfold or anything\n",
    "# Import Logistic Regression here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#clf is the logistic rgression model\n",
    "clf = LogisticRegression(C = 1, random_state=0, penalty = \"l2\").fit(X_res, y_res)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAME AS ABOVE BUT NOW USING A L1 PENALTY \n",
    "# NOTICE THIS DOES NOT RUN WITH A L1 PENALTY SINCE THE DEFAULT SOLVER IS LBFGS which only accepts a l2 penalty\n",
    "# NOT NEEDED\n",
    "\n",
    "#Just classifier no kfold or anything\n",
    "# Import Logistic Regression here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#clf is the logistic rgression model\n",
    "clf = LogisticRegression(C = 1, random_state=0, penalty = \"l1\").fit(X_res, y_res)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAME AS ABOVE BUT NOW USING A NEW SOLVER WHICH IS NEWTON Cholesky - WHICH Accepts no Penalty or only l2 penalty like lbfgs\n",
    "#OUR ACCUARACY IS SIGNIFIANTLY IMPROVED - NOW OUR ACCUARACY IS 64.8%\n",
    "#no real difference between newton\n",
    "\n",
    "#Just classifier no kfold or anything\n",
    "# Import Logistic Regression here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#clf is the logistic rgression model\n",
    "clf = LogisticRegression(solver = \"newton-cholesky\",  C = 1, random_state=0, penalty = \"l2\").fit(X_res, y_res)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAME AS ABOVE BUT NOW USING A NEW SOLVER WHICH IS NEWTON CG - WHICH Accepts no Penalty or only l2 penalty like lbfgs\n",
    "#OUR ACCUARACY IS SIGNIFIANTLY IMPROVED - NOW OUR ACCUARACY IS 64.8%\n",
    "\n",
    "#usually newtwon cg is more computationally expensive but works well on small datasets\n",
    "#like ours since its only like 25k datapoints\n",
    "#lbfgs  works  better on large datasets like million plus datapoints\n",
    "\n",
    "#Just classifier no kfold or anything\n",
    "# Import Logistic Regression here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#clf is the logistic rgression model\n",
    "clf = LogisticRegression(solver ='newton-cg', C = 1, random_state=0, penalty = \"l2\").fit(X_res, y_res)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAME AS ABOVE BUT NOW USING A NEW SOLVER WHICH IS LIBLINEAR \n",
    "#OUR ACCUARACY USING LIBLINEAR IS 62.7%\n",
    "\n",
    "#Just classifier no kfold or anything\n",
    "# Import Logistic Regression here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#clf is the logistic rgression model\n",
    "clf = LogisticRegression(solver ='liblinear', C = 1, random_state=0, penalty = \"l2\").fit(X_res, y_res)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAME AS ABOVE BUT NOW USING A NEW SOLVER WHICH IS LIBLINEAR \n",
    "#OUR ACCUARACY USING LIBLINEAR IS 64.8% whichi is the best so far\n",
    "\n",
    "#Just classifier no kfold or anything\n",
    "# Import Logistic Regression here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#clf is the logistic rgression model\n",
    "clf = LogisticRegression(solver ='liblinear', C = 1, random_state=0, penalty = \"l1\").fit(X_res, y_res)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAME AS ABOVE BUT NOW USING A NEW SOLVER WHICH IS sag\n",
    "# TERRIBLE MODEL DID NOT CONVERGE \n",
    "\n",
    "#Just classifier no kfold or anything\n",
    "# Import Logistic Regression here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#clf is the logistic rgression model\n",
    "clf = LogisticRegression(solver ='sag', C = 1, random_state=0, penalty = \"l2\").fit(X_res, y_res)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAME AS ABOVE BUT NOW USING A NEW SOLVER WHICH IS saga \n",
    "# TERRIBLE MODEL DID NOT CONVERGE \n",
    "\n",
    "#Just classifier no kfold or anything\n",
    "# Import Logistic Regression here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#clf is the logistic rgression model\n",
    "clf = LogisticRegression(solver ='saga', C = 1, random_state=0, penalty = \"l2\").fit(X_res, y_res)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAME AS ABOVE BUT NOW USING A NEW SOLVER WHICH IS saga \n",
    "# TERRIBLE MODEL DID NOT CONVERGE \n",
    "\n",
    "#Just classifier no kfold or anything\n",
    "# Import Logistic Regression here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#clf is the logistic rgression model\n",
    "clf = LogisticRegression(solver ='saga', C = 1, random_state=0, penalty = \"l1\").fit(X_res, y_res)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAME AS ABOVE BUT NOW USING A NEW SOLVER WHICH IS saga \n",
    "# TERRIBLE MODEL DID NOT CONVERGE for any of the solvers and elasticnet doesnt work for somereason\n",
    "#the reason it could not be converging could be a result of the data not being sparse enough not too sure needs more research\n",
    "\n",
    "#Just classifier no kfold or anything\n",
    "# Import Logistic Regression here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#clf is the logistic rgression model\n",
    "clf = LogisticRegression(solver ='saga', C = 1, random_state=0, penalty = \"elasticnet\").fit(X_res, y_res)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW TRYING TO USE SVM \n",
    "\n",
    "#SVM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "#from mlxtend.plotting import plot_decision_regions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR SVM TAKING TOO LONG TO RUN \n",
    "#RADIAL SVM (RBF) GIVING SOME STUPID ERROR\n",
    "\n",
    "\n",
    "# Define and Fit your Model\n",
    "model = SVC(kernel='rbf', C=1000)\n",
    "model.fit(X_res, y_res)\n",
    "\n",
    "# Plot the Data and the Decision Boundary\n",
    "plt.scatter(X_res[:, 0], X_res[:, 1], c=y_res, s=50, cmap='Spectral')\n",
    "plot_svc_decision_function(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINDING OUR BEST MODEL \n",
    "#USING SOME FEATURES TAUGHT IN LECTURE\n",
    "\n",
    "'''\n",
    "Here are the benefits and cons of each solver:\n",
    "\n",
    "'saga': This solver supports both L1 and L2 regularization and is capable of handling large datasets. It is recommended when the number of samples is significantly larger than the number of features. However, it may be slower than other solvers when the number of features is large.\n",
    "\n",
    "'lbfgs': This solver is recommended when the dataset is small or medium-sized. It is a limited-memory quasi-Newton method that uses a line search algorithm. It can handle both L1 and L2 regularization, but it may not be suitable for large datasets.\n",
    "\n",
    "'newton-cg': This solver is a quasi-Newton method that uses a Hessian matrix approximation. It can handle both L1 and L2 regularization, but it may not be suitable for large datasets.\n",
    "\n",
    "'sag': This solver is a stochastic gradient descent (SGD) algorithm. It is recommended when the dataset is large and cannot fit into memory. It is fast and can handle both L1 and L2 regularization. However, it may not converge as fast as other solvers.\n",
    "\n",
    "'newton-cholesky': This solver is a quasi-Newton method that uses a Cholesky decomposition to compute the inverse of the Hessian matrix. It can handle both L1 and L2 regularization, but it may not be suitable for large datasets.\n",
    "\n",
    "'liblinear': This solver is a linear algorithm that uses a coordinate descent method. It is recommended when the dataset is small or medium-sized and the number of samples is greater than the number of features. It can only handle L1 regularization.\n",
    "\n",
    "\n",
    "C values: smaller c-value is equal to greater generalization (simpler model) and stronger regularization\n",
    "but maybe too much while larger could mean greater overfitting (more complex model) and weaker regularization\n",
    "\n",
    "The two penalties I will leave to u to write about:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(X_res)\n",
    "categorical_columns = categorical_columns_selector(X_res)\n",
    "\n",
    "onehot = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', onehot, categorical_columns),\n",
    "    ('zscore', scaler, numerical_columns)])\n",
    "\n",
    "\n",
    "# Create a pipeline \n",
    "pipe = Pipeline([('make_features', preprocessor),\n",
    "                 ('classifier', LogisticRegression(max_iter=10000))])\n",
    "\n",
    "# Create search space of hyperparameters\n",
    "# the variable namespace looks like this\n",
    "# pipe.classifier.C is represented as 'classifier__C'\n",
    "# if we'd just chucked a LogisticRegression() in as the model\n",
    "# instead of a pipe, then we'd only have had 'C' w/o the 'classifier__' bit \n",
    "\n",
    "\n",
    "search_space = {'classifier__C': np.logspace(-4, 4, 9),\n",
    "               'classifier__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "               'classifier__solver': ['saga', 'lbfgs', 'newton-cg', 'sag', 'newton-cholesky', 'liblinear']}\n",
    "\n",
    "# Create a grid search object to find the best model\n",
    "best_model = GridSearchCV(pipe, search_space, cv=5, verbose=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "225 fits failed out of a total of 540.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "90 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 434, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Logistic Regression supports only solvers in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'], got newton-cholesky.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.93665631 0.93665631 0.93665631 0.9367313         nan 0.93630635\n",
      " 0.5               nan        nan        nan        nan 0.5\n",
      " 0.96717823 0.96717823 0.96717823 0.96717823        nan 0.96642832\n",
      " 0.97850212        nan        nan        nan        nan 0.97847712\n",
      " 0.98510151 0.98510151 0.98510151 0.98510151        nan 0.98512651\n",
      " 0.99380062        nan        nan        nan        nan 0.99387565\n",
      " 0.99280074 0.99280074 0.99280074 0.99280074        nan 0.99270075\n",
      " 0.99747526        nan        nan        nan        nan 0.99720028\n",
      " 0.99657535 0.99657535 0.99657535 0.99657535        nan 0.99652536\n",
      " 0.99955004        nan        nan        nan        nan 0.99952505\n",
      " 0.99845016 0.99860015 0.99860015 0.99855015        nan 0.99845016\n",
      " 0.99985002        nan        nan        nan        nan 0.99995001\n",
      " 0.99970003 0.99995001 0.99995001 0.99977502        nan 0.99985002\n",
      " 0.99985002        nan        nan        nan        nan 0.999975\n",
      " 0.99982502 0.999975   0.999975   0.99995001        nan 0.99992501\n",
      " 0.99985002        nan        nan        nan        nan 0.99995001\n",
      " 0.99985002 1.         1.         0.99995001        nan 0.99992501\n",
      " 0.99985002        nan        nan        nan        nan 0.99992501]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('make_features',\n",
       "                                        ColumnTransformer(transformers=[('onehot',\n",
       "                                                                         OneHotEncoder(),\n",
       "                                                                         []),\n",
       "                                                                        ('zscore',\n",
       "                                                                         StandardScaler(),\n",
       "                                                                         ['Applicant_Age',\n",
       "                                                                          'Owned_Realty',\n",
       "                                                                          'Total_Children',\n",
       "                                                                          'Owned_Car',\n",
       "                                                                          'Total_Income',\n",
       "                                                                          'Total_Family_Members',\n",
       "                                                                          'Total_Bad_Debt',\n",
       "                                                                          'Total_Good_Debt',\n",
       "                                                                          'Applicant_Gender_M      ',\n",
       "                                                                          'Housing_Type_House '\n",
       "                                                                          '/ '\n",
       "                                                                          'apartment                                 ',\n",
       "                                                                          'Housing_T...\n",
       "                                                                          'Family_Status_Single '\n",
       "                                                                          '/ '\n",
       "                                                                          'not '\n",
       "                                                                          'married                              ',\n",
       "                                                                          'Family_Status_Widow                                             '])])),\n",
       "                                       ('classifier',\n",
       "                                        LogisticRegression(max_iter=10000))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'classifier__C': array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03,\n",
       "       1.e+04]),\n",
       "                         'classifier__penalty': ['l2', 'l1'],\n",
       "                         'classifier__solver': ['saga', 'lbfgs', 'newton-cg',\n",
       "                                                'sag', 'newton-cholesky',\n",
       "                                                'liblinear']},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FITTING OUR BEST MODEL \n",
    "#FINDING THE BEST ACCUARACY \n",
    "#why are we splitting again here? shouldnt we fit on res anyways\n",
    "\n",
    "\n",
    "# Fit grid search\n",
    "best_model.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'classifier__C': 10000.0,\n",
       "  'classifier__penalty': 'l2',\n",
       "  'classifier__solver': 'lbfgs'},\n",
       " 1.0)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS OUR BEST MODEL \n",
    "#IT HAS C = 1, PENALTY = L2, ACCUARACY SCORE OF 67.94%\n",
    "#USING CROSS VALIDATION \n",
    "#--------------------------------------------------------From neil before adding bad and good debt\n",
    "\n",
    "#after adding bad and good debt and removing the previous categorical values\n",
    "#THe best model is Solver = 'lbfgs',  C = 1000.0,  PENALTY = l2, and accuracy of 100\n",
    "\n",
    "# hay our best model has these params, and it turns out its cross validation score was perfect!\n",
    "# note how sklearn does this... once it determines the best version of the classifier\n",
    "# it refits those parameters on ALL of `X_train`... \n",
    "# in other worsds it doesn't just leave the last cross validaiton fold version hanging around :)\n",
    "best_model.best_params_, best_model.best_score_ #maybe too good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Index contains duplicate entries, cannot reshape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17916\\3253857247.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# lets show the results across penalty & C values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m sns.heatmap( results.pivot('C','penalty','accuracy'),\n\u001b[0m\u001b[0;32m     18\u001b[0m              annot=True, fmt='3.2f')\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mpivot\u001b[1;34m(self, index, columns, values)\u001b[0m\n\u001b[0;32m   7883\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpivot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7884\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7885\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpivot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7887\u001b[0m     _shared_docs[\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py\u001b[0m in \u001b[0;36mpivot\u001b[1;34m(data, index, columns, values)\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[0mindexed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmultiindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mindexed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns_listlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36munstack\u001b[1;34m(self, level, fill_value)\u001b[0m\n\u001b[0;32m   4155\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0munstack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4157\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0munstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4159\u001b[0m     \u001b[1;31m# ----------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py\u001b[0m in \u001b[0;36munstack\u001b[1;34m(obj, level, fill_value)\u001b[0m\n\u001b[0;32m    489\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_1d_only_ea_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_unstack_extension_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m         unstacker = _Unstacker(\n\u001b[0m\u001b[0;32m    492\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstructor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_expanddim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, index, level, constructor)\u001b[0m\n\u001b[0;32m    138\u001b[0m             )\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_selectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcache_readonly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py\u001b[0m in \u001b[0;36m_make_selectors\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Index contains duplicate entries, cannot reshape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomp_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Index contains duplicate entries, cannot reshape"
     ]
    }
   ],
   "source": [
    "#PREVIEWING EVERY MODEL WE CHECKED\n",
    "#PRINTING OUT RESULTS THAT CONSIST OF C VALUE, PENALTY AND ACCURACY \n",
    "#USING CHART TO DISPLAY OUR RESULTS\n",
    "\n",
    "\n",
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we could use 1 - x\n",
    "results['accuracy'] = best_model.cv_results_['mean_test_score']\n",
    "\n",
    "# get rid of classifier__XX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results across penalty & C values\n",
    "sns.heatmap( results.pivot('C','penalty','accuracy'), \n",
    "             annot=True, fmt='3.2f')\n",
    "\n",
    "results\n",
    "\n",
    "#a possible solution to solve the duplicate indexes: \n",
    "'''\n",
    "The error message indicates that there are duplicate entries in the index, which is causing the pivot() function to fail.\n",
    "\n",
    "You can try removing any duplicate entries in the index by running:\n",
    "\n",
    "css\n",
    "Copy code\n",
    "results = results[~results.index.duplicated(keep='first')]\n",
    "This will keep the first occurrence of any duplicate index values and remove the subsequent ones. \n",
    "After removing duplicates, you can try running the heatmap() function again.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average difference of 0.000001 with std. dev. of 0.000005.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#NOT A MODEL JUST COMPARING TWO DIFFERENT MODELS TO SEE WHICH INE IS BETTER\n",
    "\n",
    "# this code is from sklearn docs\n",
    "# 20x it does both a non-nested and a nested cross val on the same data\n",
    "# the two techniques are compared in their average error across the 20x loop\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Number of random trials\n",
    "NUM_TRIALS = 20\n",
    "\n",
    "\n",
    "# Set up possible values of parameters to optimize over\n",
    "# this is a regularization parameter, smaller C => more regularization\n",
    "p_grid = {\"classifier__C\": [1, 10, 100,  1000]}\n",
    "\n",
    "\n",
    "# Arrays to store scores\n",
    "non_nested_scores = np.zeros(NUM_TRIALS)\n",
    "nested_scores = np.zeros(NUM_TRIALS)\n",
    "\n",
    "# Loop for each trial\n",
    "for i in range(NUM_TRIALS):\n",
    "\n",
    "    # Choose cross-validation techniques for the inner and outer loops,\n",
    "    # independently of the dataset.\n",
    "    # E.g \"GroupKFold\", \"LeaveOneOut\", \"LeaveOneGroupOut\", etc.\n",
    "    inner_cv = KFold(n_splits=3, shuffle=True, random_state=i)\n",
    "    outer_cv = KFold(n_splits=3, shuffle=True, random_state=i)\n",
    "\n",
    "    # Non_nested parameter search and scoring\n",
    "    clf = GridSearchCV(estimator=pipe, param_grid=p_grid, cv=inner_cv)\n",
    "    clf.fit(X_res, y_res)\n",
    "    non_nested_scores[i] = clf.best_score_\n",
    "\n",
    "    # Nested CV with parameter optimization\n",
    "    nested_score = cross_val_score(clf, X=X_res, y=y_res, cv=outer_cv)\n",
    "    nested_scores[i] = nested_score.mean()\n",
    "\n",
    "score_difference = non_nested_scores - nested_scores\n",
    "\n",
    "print(\"Average difference of {:6f} with std. dev. of {:6f}.\"\n",
    "      .format(score_difference.mean(), score_difference.std()))\n",
    "\n",
    "# Plot scores on each trial for nested and non-nested CV\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "non_nested_scores_line, = plt.plot(non_nested_scores, color='r')\n",
    "nested_line, = plt.plot(nested_scores, color='b')\n",
    "plt.ylabel(\"score\", fontsize=\"14\")\n",
    "plt.legend([non_nested_scores_line, nested_line],\n",
    "           [\"Non-Nested CV\", \"Nested CV\"],\n",
    "           bbox_to_anchor=(1, 0, 0, .8)) #(0, .4, .5, 0))\n",
    "plt.title(\"Non-Nested and Nested Cross Validation predicting penguin species from measurements\",\n",
    "          x=.5, y=1.1, fontsize=\"15\")\n",
    "\n",
    "# Plot bar chart of the difference.\n",
    "plt.subplot(212)\n",
    "difference_plot = plt.bar(range(NUM_TRIALS), score_difference)\n",
    "plt.xlabel(\"Individual Trial #\")\n",
    "plt.legend([difference_plot],\n",
    "           [\"Non-Nested CV - Nested CV Score\"],\n",
    "           bbox_to_anchor=(1, 0, 0, .8))\n",
    "plt.ylabel(\"score difference\", fontsize=\"14\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will do this for svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We also need to explain why we choose svc rather than svm's other classifiers in sklearn\n",
    "\n",
    "\n",
    "'classifier__C': np.logspace(-4, 4, 9), <- just the usual C value stuff\n",
    "               'classifier__kernel': ['linear', 'rbf', 'sigmoid', 'poly'], <-\n",
    "        'linear': This kernel is a linear transformation that maps the data to a higher-dimensional \n",
    "        space using a linear function. It is suitable for linearly separable datasets and works well \n",
    "        when there are many features. The 'linear' kernel is computationally efficient and is less prone\n",
    "        to overfitting than other kernels. However, it may not work well on nonlinear datasets.\n",
    "\n",
    "        'rbf' (Radial basis function): This kernel is a popular choice for nonlinear datasets. \n",
    "        It maps the data to a high-dimensional feature space using a Gaussian function. \n",
    "        It can capture complex relationships between features and is very flexible. \n",
    "        However, the 'rbf' kernel can be sensitive to the choice of hyperparameters, such as the \n",
    "        width of the Gaussian function, and can overfit when the number of features is large.\n",
    "\n",
    "        'sigmoid': This kernel maps the data to a high-dimensional feature space using a sigmoid \n",
    "        function. It is suitable for problems that require a logistic function as a decision function.\n",
    "        However, the 'sigmoid' kernel is less popular than the other kernels and may not perform as \n",
    "        well on many datasets.\n",
    "\n",
    "        'poly' (Polynomial): This kernel maps the data to a high-dimensional feature space using \n",
    "        a polynomial function. It is suitable for datasets with nonlinear relationships between \n",
    "        features. However, the 'poly' kernel can be sensitive to the degree of the polynomial \n",
    "        and can overfit when the degree is too high. <- was the best before i had to refit the data not sure whats happening\n",
    "        rn\n",
    "               \n",
    "               \n",
    "               \n",
    "               \n",
    "               \n",
    "            'classifier__gamma': ['scale', 'auto', 0.1, 1, 10] <-Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.\n",
    "\n",
    "if gamma='scale' (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma,\n",
    "\n",
    "if ‘auto’, uses 1 / n_features\n",
    "\n",
    "if float, must be non-negative.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(X_res)\n",
    "categorical_columns = categorical_columns_selector(X_res)\n",
    "\n",
    "onehot = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', onehot, categorical_columns),\n",
    "    ('zscore', scaler, numerical_columns)])\n",
    "\n",
    "\n",
    "# Create a pipeline \n",
    "pipe = Pipeline([('make_features', preprocessor),\n",
    "                 ('classifier', SVC())])\n",
    "\n",
    "# Create search space of hyperparameters\n",
    "# the variable namespace looks like this\n",
    "# pipe.classifier.C is represented as 'classifier__C'\n",
    "# if we'd just chucked a LogisticRegression() in as the model\n",
    "# instead of a pipe, then we'd only have had 'C' w/o the 'classifier__' bit \n",
    "search_space = {'classifier__C': np.logspace(-4, 4, 9),\n",
    "               'classifier__kernel': ['linear', 'rbf', 'sigmoid', 'poly'],\n",
    "               'classifier__gamma': ['scale', 'auto', 0.1, 1, 10]}\n",
    "\n",
    "# Create a grid search object to find the best model\n",
    "best_model = GridSearchCV(pipe, search_space, cv=5, verbose=3, n_jobs=-1) #idk why the verbose isnt showing for me anymore\n",
    "#it just runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('make_features',\n",
       "                                        ColumnTransformer(transformers=[('onehot',\n",
       "                                                                         OneHotEncoder(),\n",
       "                                                                         []),\n",
       "                                                                        ('zscore',\n",
       "                                                                         StandardScaler(),\n",
       "                                                                         ['Applicant_Age',\n",
       "                                                                          'Owned_Realty',\n",
       "                                                                          'Total_Children',\n",
       "                                                                          'Owned_Car',\n",
       "                                                                          'Total_Income',\n",
       "                                                                          'Total_Family_Members',\n",
       "                                                                          'Total_Bad_Debt',\n",
       "                                                                          'Total_Good_Debt',\n",
       "                                                                          'Applicant_Gender_M      ',\n",
       "                                                                          'Housing_Type_House '\n",
       "                                                                          '/ '\n",
       "                                                                          'apartment                                 ',\n",
       "                                                                          'Housing_T...\n",
       "                                                                          'Family_Status_Married                                           ',\n",
       "                                                                          'Family_Status_Separated                                         ',\n",
       "                                                                          'Family_Status_Single '\n",
       "                                                                          '/ '\n",
       "                                                                          'not '\n",
       "                                                                          'married                              ',\n",
       "                                                                          'Family_Status_Widow                                             '])])),\n",
       "                                       ('classifier', SVC())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'classifier__C': array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03,\n",
       "       1.e+04]),\n",
       "                         'classifier__gamma': ['scale', 'auto', 0.1, 1, 10],\n",
       "                         'classifier__kernel': ['linear', 'rbf', 'sigmoid',\n",
       "                                                'poly']},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FITTING OUR BEST MODEL \n",
    "#FINDING THE BEST ACCUARACY \n",
    "#why are we splitting again here? shouldnt we fit on res anyways\n",
    "\n",
    "#_-----------------------------------------------------------------------------------\n",
    "#X_train2, X_test2, y_train2, y_test2 = train_test_split(X_res, y_res, \n",
    "                                                    #test_size=0.2, random_state=101)\n",
    "\n",
    "#_------------------------------------------------------------------------------------\n",
    "# Fit grid search\n",
    "\n",
    "\n",
    "best_model.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'classifier__C': 10.0,\n",
       "  'classifier__gamma': 'scale',\n",
       "  'classifier__kernel': 'linear'},\n",
       " 1.0)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS OUR BEST MODEL \n",
    "#IT HAS C = 1, PENALTY = L2, ACCUARACY SCORE OF 67.94%--> improved to 68.6% according to above \n",
    "#USING CROSS VALIDATION \n",
    "\n",
    "\n",
    "# hay our best model has these params, and it turns out its cross validation score was perfect!\n",
    "# note how sklearn does this... once it determines the best version of the classifier\n",
    "# it refits those parameters on ALL of `X_train`... \n",
    "# in other worsds it doesn't just leave the last cross validaiton fold version hanging around :)\n",
    "best_model.best_params_, best_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT A MODEL JUST COMPARING TWO DIFFERENT MODELS TO SEE WHICH INE IS BETTER\n",
    "\n",
    "# this code is from sklearn docs\n",
    "# 20x it does both a non-nested and a nested cross val on the same data\n",
    "# the two techniques are compared in their average error across the 20x loop\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Number of random trials\n",
    "NUM_TRIALS = 20\n",
    "\n",
    "\n",
    "# Set up possible values of parameters to optimize over\n",
    "# this is a regularization parameter, smaller C => more regularization\n",
    "p_grid = {\"classifier__C\": [1, 10, 100,  1000]}\n",
    "\n",
    "\n",
    "# Arrays to store scores\n",
    "non_nested_scores = np.zeros(NUM_TRIALS)\n",
    "nested_scores = np.zeros(NUM_TRIALS)\n",
    "\n",
    "# Loop for each trial\n",
    "for i in range(NUM_TRIALS):\n",
    "\n",
    "    # Choose cross-validation techniques for the inner and outer loops,\n",
    "    # independently of the dataset.\n",
    "    # E.g \"GroupKFold\", \"LeaveOneOut\", \"LeaveOneGroupOut\", etc.\n",
    "    inner_cv = KFold(n_splits=3, shuffle=True, random_state=i)\n",
    "    outer_cv = KFold(n_splits=3, shuffle=True, random_state=i)\n",
    "\n",
    "    # Non_nested parameter search and scoring\n",
    "    clf = GridSearchCV(estimator=pipe, param_grid=p_grid, cv=inner_cv)\n",
    "    clf.fit(X_res, y_res)\n",
    "    non_nested_scores[i] = clf.best_score_\n",
    "\n",
    "    # Nested CV with parameter optimization\n",
    "    nested_score = cross_val_score(clf, X=X_res, y=y_res, cv=outer_cv)\n",
    "    nested_scores[i] = nested_score.mean()\n",
    "\n",
    "score_difference = non_nested_scores - nested_scores\n",
    "\n",
    "print(\"Average difference of {:6f} with std. dev. of {:6f}.\"\n",
    "      .format(score_difference.mean(), score_difference.std()))\n",
    "\n",
    "# Plot scores on each trial for nested and non-nested CV\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "non_nested_scores_line, = plt.plot(non_nested_scores, color='r')\n",
    "nested_line, = plt.plot(nested_scores, color='b')\n",
    "plt.ylabel(\"score\", fontsize=\"14\")\n",
    "plt.legend([non_nested_scores_line, nested_line],\n",
    "           [\"Non-Nested CV\", \"Nested CV\"],\n",
    "           bbox_to_anchor=(1, 0, 0, .8)) #(0, .4, .5, 0))\n",
    "plt.title(\"Non-Nested and Nested Cross Validation predicting penguin species from measurements\",\n",
    "          x=.5, y=1.1, fontsize=\"15\")\n",
    "\n",
    "# Plot bar chart of the difference.\n",
    "plt.subplot(212)\n",
    "difference_plot = plt.bar(range(NUM_TRIALS), score_difference)\n",
    "plt.xlabel(\"Individual Trial #\")\n",
    "plt.legend([difference_plot],\n",
    "           [\"Non-Nested CV - Nested CV Score\"],\n",
    "           bbox_to_anchor=(1, 0, 0, .8))\n",
    "plt.ylabel(\"score difference\", fontsize=\"14\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "#next is decision tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For classifier__criterion: \n",
    "'gini' index: The Gini index measures the impurity of a node in a decision tree. \n",
    "It is a measure of the probability of misclassifying a randomly chosen element from the set. \n",
    "A lower Gini index indicates a better split. The advantages of using the Gini index are that \n",
    "it is computationally efficient and works well with categorical and continuous data. However, \n",
    "the Gini index may not perform well when the classes are imbalanced.\n",
    "\n",
    "'entropy' criterion: The entropy criterion measures the impurity of a node by calculating the\n",
    "entropy of the class distribution. A lower entropy indicates a better split. \n",
    "The advantages of using the entropy criterion are that it is computationally efficient and \n",
    "works well with categorical and continuous data. The entropy criterion is also more sensitive to \n",
    "changes in class probabilities than the Gini index. However, like the Gini index, the entropy criterion \n",
    "may not perform well when the classes are imbalanced. \n",
    "\n",
    "#our data was imbalanced originally and we attempted to fix that by oversampling using smote technique\n",
    "\n",
    "'log_loss' criterion: The log loss criterion is a measure of the error rate of a decision tree. \n",
    "It is based on the logistic loss function and is commonly used in logistic regression. \n",
    "A lower log loss indicates a better split. The advantages of using the log loss criterion \n",
    "are that it can handle imbalanced classes and is less prone to overfitting than the Gini index \n",
    "or entropy criterion. However, the log loss criterion is computationally more expensive and may \n",
    "not work well with categorical data.  #before oversampling this would have been a very good model\n",
    "\n",
    "\n",
    "In summary, the choice of criterion depends on the specific problem and the available data. \n",
    "The Gini index and entropy criterion are computationally efficient and work well with categorical \n",
    "and continuous data. The log loss criterion can handle imbalanced classes and is less prone to overfitting \n",
    "but is computationally more expensive and may not work well with categorical data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For depth: None allows it to fit as much as it wants and the less the simpler the model (e.g. 2) and the more\n",
    "(e.g. 10) the more complex and prone to overfitting\n",
    "\n",
    "\n",
    "\n",
    "for min samples split: the smaller more splits and deeper the tree is allowed to be (prone to overfitting and more complex)\n",
    "and the larger the splits teh shallower and less splits allowed so (prone to underfitting and less complex)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For min_samples_leaf: same as above for split\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(X_res)\n",
    "categorical_columns = categorical_columns_selector(X_res)\n",
    "\n",
    "onehot = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', onehot, categorical_columns),\n",
    "    ('zscore', scaler, numerical_columns)])\n",
    "\n",
    "# Create a pipeline \n",
    "pipe = Pipeline([('make_features', preprocessor),\n",
    "                 ('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "# Create search space of hyperparameters\n",
    "search_space = {'classifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "                'classifier__max_depth': [None, 2, 5, 10, 20],\n",
    "                'classifier__min_samples_split': [2, 5, 10],\n",
    "                'classifier__min_samples_leaf': [1, 2, 4]}\n",
    "\n",
    "# Create a grid search object to find the best model\n",
    "best_model = GridSearchCV(pipe, search_space, cv=5, verbose=3, n_jobs=-1) #njobs = -1 makes it faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 135 candidates, totalling 675 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "225 fits failed out of a total of 675.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "225 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.99960005 0.99955005 0.99947506 0.99937507 0.99937507 0.99947506\n",
      " 0.99942506 0.99942506 0.99942506 0.9847765  0.9847765  0.9847765\n",
      " 0.9847765  0.9847765  0.9847765  0.9847765  0.9847765  0.9847765\n",
      " 0.99565047 0.99560047 0.99562547 0.99562547 0.99562547 0.99562547\n",
      " 0.99550048 0.99550048 0.99550048 0.99935007 0.99932507 0.99932507\n",
      " 0.99925008 0.99932507 0.99932507 0.99927508 0.99927508 0.99927508\n",
      " 0.99947506 0.99952506 0.99955005 0.99935007 0.99935007 0.99942506\n",
      " 0.99942506 0.99942506 0.99942506 0.99975003 0.99977502 0.99975003\n",
      " 0.99967504 0.99970004 0.99970003 0.99970003 0.99970003 0.99970003\n",
      " 0.97490252 0.97490252 0.97490252 0.97490252 0.97490252 0.97490252\n",
      " 0.97490252 0.97490252 0.97490252 0.99825018 0.99820018 0.99822518\n",
      " 0.99820018 0.99822518 0.99822518 0.99822518 0.99822518 0.99822518\n",
      " 0.99977502 0.99975003 0.99980002 0.99975003 0.99962504 0.99975003\n",
      " 0.99975003 0.99970003 0.99975003 0.99977503 0.99980002 0.99975003\n",
      " 0.99975003 0.99972503 0.99975003 0.99975003 0.99975003 0.99975003\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('make_features',\n",
       "                                        ColumnTransformer(transformers=[('onehot',\n",
       "                                                                         OneHotEncoder(),\n",
       "                                                                         []),\n",
       "                                                                        ('zscore',\n",
       "                                                                         StandardScaler(),\n",
       "                                                                         ['Applicant_Age',\n",
       "                                                                          'Owned_Realty',\n",
       "                                                                          'Total_Children',\n",
       "                                                                          'Owned_Car',\n",
       "                                                                          'Total_Income',\n",
       "                                                                          'Total_Family_Members',\n",
       "                                                                          'Total_Bad_Debt',\n",
       "                                                                          'Total_Good_Debt',\n",
       "                                                                          'Applicant_Gender_M      ',\n",
       "                                                                          'Housing_Type_House '\n",
       "                                                                          '/ '\n",
       "                                                                          'apartment                                 ',\n",
       "                                                                          'Housing_T...\n",
       "                                                                          'Family_Status_Married                                           ',\n",
       "                                                                          'Family_Status_Separated                                         ',\n",
       "                                                                          'Family_Status_Single '\n",
       "                                                                          '/ '\n",
       "                                                                          'not '\n",
       "                                                                          'married                              ',\n",
       "                                                                          'Family_Status_Widow                                             '])])),\n",
       "                                       ('classifier',\n",
       "                                        DecisionTreeClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'classifier__criterion': ['gini', 'entropy',\n",
       "                                                   'log_loss'],\n",
       "                         'classifier__max_depth': [None, 2, 5, 10, 20],\n",
       "                         'classifier__min_samples_leaf': [1, 2, 4],\n",
       "                         'classifier__min_samples_split': [2, 5, 10]},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FITTING OUR BEST MODEL \n",
    "#FINDING THE BEST ACCUARACY \n",
    "#why are we splitting again here? shouldnt we fit on res anyways\n",
    "\n",
    "\n",
    "# Fit grid search\n",
    "\n",
    "%timeit\n",
    "\n",
    "best_model.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'classifier__criterion': 'entropy',\n",
       "  'classifier__max_depth': 10,\n",
       "  'classifier__min_samples_leaf': 1,\n",
       "  'classifier__min_samples_split': 10},\n",
       " 0.999800021872266)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS OUR BEST MODEL \n",
    "#IT HAS C = 1, PENALTY = L2, ACCUARACY SCORE OF 67.94%--> improved to 68.6% according to above \n",
    "#USING CROSS VALIDATION \n",
    "\n",
    "\n",
    "# hay our best model has these params, and it turns out its cross validation score was perfect!\n",
    "# note how sklearn does this... once it determines the best version of the classifier\n",
    "# it refits those parameters on ALL of `X_train`... \n",
    "# in other worsds it doesn't just leave the last cross validaiton fold version hanging around :)\n",
    "best_model.best_params_, best_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "#next is naive bayes whihch doenst have any hyperparameters to tune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Naive bayes has no hyperparameter so just explain naive bayes itself\n",
    "'''\n",
    "\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Define numerical and categorical column selectors\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "# Get numerical and categorical column names\n",
    "numerical_columns = numerical_columns_selector(X_res)\n",
    "categorical_columns = categorical_columns_selector(X_res)\n",
    "\n",
    "# Define one-hot encoder and scaler\n",
    "onehot = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', onehot, categorical_columns),\n",
    "    ('zscore', scaler, numerical_columns)\n",
    "])\n",
    "\n",
    "# Create a pipeline \n",
    "pipe = Pipeline([\n",
    "    ('make_features', preprocessor),\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "# Create search space of hyperparameters\n",
    "search_space = {}\n",
    "\n",
    "# Create a grid search object to find the best model\n",
    "best_model = GridSearchCV(pipe, search_space, cv=5, verbose=3, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('make_features',\n",
       "                                        ColumnTransformer(transformers=[('onehot',\n",
       "                                                                         OneHotEncoder(),\n",
       "                                                                         []),\n",
       "                                                                        ('zscore',\n",
       "                                                                         StandardScaler(),\n",
       "                                                                         ['Applicant_Age',\n",
       "                                                                          'Owned_Realty',\n",
       "                                                                          'Total_Children',\n",
       "                                                                          'Owned_Car',\n",
       "                                                                          'Total_Income',\n",
       "                                                                          'Total_Family_Members',\n",
       "                                                                          'Total_Bad_Debt',\n",
       "                                                                          'Total_Good_Debt',\n",
       "                                                                          'Applicant_Gender_M      ',\n",
       "                                                                          'Housing_Type_House '\n",
       "                                                                          '/ '\n",
       "                                                                          'apartment                                 ',\n",
       "                                                                          'Housing_T...\n",
       "                                                                          'Housing_Type_With '\n",
       "                                                                          'parents                                      ',\n",
       "                                                                          'Education_Type_Higher '\n",
       "                                                                          'education                                  ',\n",
       "                                                                          'Education_Type_Incomplete '\n",
       "                                                                          'higher                                 ',\n",
       "                                                                          'Education_Type_Lower '\n",
       "                                                                          'secondary                                   ',\n",
       "                                                                          'Education_Type_Secondary '\n",
       "                                                                          '/ '\n",
       "                                                                          'secondary '\n",
       "                                                                          'special                     ',\n",
       "                                                                          'Family_Status_Married                                           ',\n",
       "                                                                          'Family_Status_Separated                                         ',\n",
       "                                                                          'Family_Status_Single '\n",
       "                                                                          '/ '\n",
       "                                                                          'not '\n",
       "                                                                          'married                              ',\n",
       "                                                                          'Family_Status_Widow                                             '])])),\n",
       "                                       ('classifier', GaussianNB())]),\n",
       "             n_jobs=-1, param_grid={}, verbose=3)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FITTING OUR BEST MODEL \n",
    "#FINDING THE BEST ACCUARACY \n",
    "#why are we splitting again here? shouldnt we fit on res anyways\n",
    "\n",
    "\n",
    "# Fit grid search\n",
    "best_model.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, 0.9106838707661542)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS OUR BEST MODEL \n",
    "#IT HAS C = 1, PENALTY = L2, ACCUARACY SCORE OF 67.94%--> improved to 68.6% according to above \n",
    "#USING CROSS VALIDATION \n",
    "\n",
    "\n",
    "# hay our best model has these params, and it turns out its cross validation score was perfect!\n",
    "# note how sklearn does this... once it determines the best version of the classifier\n",
    "# it refits those parameters on ALL of `X_train`... \n",
    "# in other worsds it doesn't just leave the last cross validaiton fold version hanging around :)\n",
    "best_model.best_params_, best_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# next is knn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "the number of neighbhors the more the neighbhors the more the \n",
    "\n",
    "the weights are quite literally that uniform is just 1/n and distance values the closer the neighbors \n",
    "the more value it willhave\n",
    "\n",
    "finally the algorithms: \n",
    "'auto': This is the default algorithm, which selects the most appropriate algorithm \n",
    "based on the characteristics of the data. It is a good choice for most datasets.\n",
    "\n",
    "Pros:\n",
    "\n",
    "Automatic selection of the best algorithm based on the input data.\n",
    "Suitable for most datasets.\n",
    "Cons:\n",
    "\n",
    "None.\n",
    "\n",
    "\n",
    "'ball_tree': This algorithm constructs a ball tree data structure to find the nearest \n",
    "neighbors. It is a good choice when the number of features is large compared to the number of samples. #not good for ours\n",
    "\n",
    "Pros:\n",
    "\n",
    "Efficient for high-dimensional datasets.\n",
    "Can be faster than brute force algorithm for some datasets.\n",
    "Cons:\n",
    "\n",
    "Slower than brute force algorithm for low-dimensional datasets. Memory-intensive.\n",
    "\n",
    "\n",
    "'kd_tree': This algorithm constructs a kd-tree data structure to find the nearest neighbors. \n",
    "It is a good choice when the number of features is small compared to the number of samples. #good for ours\n",
    "\n",
    "Pros:\n",
    "\n",
    "Efficient for low-dimensional datasets.\n",
    "Can be faster than brute force algorithm for some datasets.\n",
    "Cons:\n",
    "\n",
    "Slower than brute force algorithm for high-dimensional datasets.\n",
    "Memory-intensive.\n",
    "\n",
    "\n",
    "'brute': This algorithm computes the distances between all pairs of points in the dataset to find the nearest\n",
    "neighbors. It is a good choice for small datasets. #ok for ours i think kd-tree is better\n",
    "\n",
    "Pros:\n",
    "\n",
    "Simple and easy to understand.\n",
    "Suitable for small datasets.\n",
    "Cons:\n",
    "\n",
    "Slow and inefficient for large datasets. Memory-intensive.\n",
    "\n",
    "In general, the choice of algorithm depends on the size and characteristics of the dataset. \n",
    "If the number of features is large, ball tree algorithm may be a good choice, whereas if the number of samples is small and the number of features is low, kd-tree algorithm may be a good choice. If the dataset is small, brute force algorithm may be a good choice. The auto option can be a good\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Define numerical and categorical column selectors\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "# Get numerical and categorical column names\n",
    "numerical_columns = numerical_columns_selector(X_res)\n",
    "categorical_columns = categorical_columns_selector(X_res)\n",
    "\n",
    "# Define one-hot encoder and scaler\n",
    "onehot = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', onehot, categorical_columns),\n",
    "    ('zscore', scaler, numerical_columns)\n",
    "])\n",
    "\n",
    "# Create a pipeline \n",
    "pipe = Pipeline([\n",
    "    ('make_features', preprocessor),\n",
    "    ('classifier', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Create search space of hyperparameters\n",
    "search_space = {\n",
    "    'classifier__n_neighbors': range(1, 31),\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "}\n",
    "\n",
    "# Create a grid search object to find the best model\n",
    "best_model = GridSearchCV(pipe, search_space, cv=5, verbose=3, n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FITTING OUR BEST MODEL \n",
    "#FINDING THE BEST ACCUARACY \n",
    "#why are we splitting again here? shouldnt we fit on res anyways\n",
    "\n",
    "\n",
    "# Fit grid search\n",
    "best_model.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS OUR BEST MODEL \n",
    "#IT HAS C = 1, PENALTY = L2, ACCUARACY SCORE OF 67.94%--> improved to 68.6% according to above \n",
    "#USING CROSS VALIDATION \n",
    "\n",
    "\n",
    "# hay our best model has these params, and it turns out its cross validation score was perfect!\n",
    "# note how sklearn does this... once it determines the best version of the classifier\n",
    "# it refits those parameters on ALL of `X_train`... \n",
    "# in other worsds it doesn't just leave the last cross validaiton fold version hanging around :)\n",
    "best_model.best_params_, best_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "#next is kernel ridge regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "We covered the kernel values here \n",
    "\n",
    "the gamma value  determines the inverse of the width of the kernel function, \n",
    "which in turn affects the degree of smoothing of the resulting regression function.\n",
    "\n",
    "The 'scale' value of the regressor__gamma hyperparameter specifies \n",
    "that gamma should be set to 1 / (n_features * X.var()), where n_features is \n",
    "the number of features in the input data X. This is equivalent to scaling each feature \n",
    "to have unit variance before computing gamma.\n",
    "\n",
    "The 'auto' value of regressor__gamma specifies that gamma should be set to 1 / n_features. \n",
    "This is equivalent to using the median of the distances between all pairs of points in the \n",
    "input data X to set the scale of the kernel function.\n",
    "\n",
    "Finally the alpha value a lower value like 0.001 means weak regularization and more complex model\n",
    "with greater overfittign and vice versa for values like a 1000\n",
    "'''\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Define numerical and categorical column selectors\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "# Get numerical and categorical column names\n",
    "numerical_columns = numerical_columns_selector(X_res)\n",
    "categorical_columns = categorical_columns_selector(X_res)\n",
    "\n",
    "# Define one-hot encoder and scaler\n",
    "onehot = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', onehot, categorical_columns),\n",
    "    ('zscore', scaler, numerical_columns)\n",
    "])\n",
    "\n",
    "# Create a pipeline \n",
    "pipe = Pipeline([\n",
    "    ('make_features', preprocessor),\n",
    "    ('regressor', KernelRidge())\n",
    "])\n",
    "\n",
    "# Create search space of hyperparameters\n",
    "search_space = {\n",
    "    'regressor__alpha': [0.1, 1, 10],\n",
    "    'regressor__kernel': ['linear', 'rbf', 'polynomial'],\n",
    "    'regressor__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Create a grid search object to find the best model\n",
    "best_model = GridSearchCV(pipe, search_space, cv=5, verbose=3, n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FITTING OUR BEST MODEL \n",
    "#FINDING THE BEST ACCUARACY \n",
    "#why are we splitting again here? shouldnt we fit on res anyways\n",
    "\n",
    "\n",
    "# Fit grid search\n",
    "\n",
    "%timeit\n",
    "best_model.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS OUR BEST MODEL \n",
    "#IT HAS C = 1, PENALTY = L2, ACCUARACY SCORE OF 67.94%--> improved to 68.6% according to above \n",
    "#USING CROSS VALIDATION \n",
    "\n",
    "\n",
    "# hay our best model has these params, and it turns out its cross validation score was perfect!\n",
    "# note how sklearn does this... once it determines the best version of the classifier\n",
    "# it refits those parameters on ALL of `X_train`... \n",
    "# in other worsds it doesn't just leave the last cross validaiton fold version hanging around :)\n",
    "best_model.best_params_, best_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#only thing left are the ensemble methods ---Random Forest, Adaboost, Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "### Next is Adaaptive Gradient boost Ensemble Technique also known as adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "n_estimators is the max number of estimators (weak learners) the value is capped at in this case and higher number is more \n",
    "complex (like 200) and lower is less and 50 is the default value\n",
    "\n",
    "learning rate is teh step size and lower is more complex and prone to overfitting and higher isnt\n",
    "\n",
    "\n",
    "the algorithms are used to calculate the weights for each of the weak learners in the ensemble.\n",
    "\n",
    "'SAMME' stands for Stagewise Additive Modeling using a Multi-class Exponential loss function. \n",
    "This algorithm works well for binary classification problems, but can also be extended to multiclass problems.\n",
    "'SAMME.R' is a variant of the SAMME algorithm that uses real-valued predictions \n",
    "from the weak learners and can often converge faster than the original SAMME algorithm.\n",
    "In general, 'SAMME.R' tends to perform better than 'SAMME' for most cases, but the \n",
    "optimal choice may depend on the specific problem being solved.\n",
    "\n",
    "#both of the algorithms seem reasonable to me\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Define numerical and categorical column selectors\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "# Get numerical and categorical column names\n",
    "numerical_columns = numerical_columns_selector(X_res)\n",
    "categorical_columns = categorical_columns_selector(X_res)\n",
    "\n",
    "# Define one-hot encoder and scaler\n",
    "onehot = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', onehot, categorical_columns),\n",
    "    ('zscore', scaler, numerical_columns)\n",
    "])\n",
    "\n",
    "# Create a pipeline \n",
    "pipe = Pipeline([\n",
    "    ('make_features', preprocessor),\n",
    "    ('classifier', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "# Create search space of hyperparameters\n",
    "search_space = {\n",
    "    'classifier__n_estimators': [50, 100, 150, 200],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 1, 10],\n",
    "    'classifier__algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "\n",
    "# Create a grid search object to find the best model\n",
    "best_model = GridSearchCV(pipe, search_space, cv=5, verbose=3, n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FITTING OUR BEST MODEL \n",
    "#FINDING THE BEST ACCUARACY \n",
    "#why are we splitting again here? shouldnt we fit on res anyways\n",
    "\n",
    "\n",
    "# Fit grid search\n",
    "%timeit\n",
    "best_model.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS OUR BEST MODEL \n",
    "#IT HAS C = 1, PENALTY = L2, ACCUARACY SCORE OF 67.94%--> improved to 68.6% according to above \n",
    "#USING CROSS VALIDATION \n",
    "\n",
    "\n",
    "# hay our best model has these params, and it turns out its cross validation score was perfect!\n",
    "# note how sklearn does this... once it determines the best version of the classifier\n",
    "# it refits those parameters on ALL of `X_train`... \n",
    "# in other worsds it doesn't just leave the last cross validaiton fold version hanging around :)\n",
    "best_model.best_params_, best_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "## Next is Gradient Tree Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "estimators and learning rate i covered above\n",
    "and since its a tree we know what the depth, leaves, and split  means as well\n",
    "only the max features are left:\n",
    "\n",
    "The max_features parameter in gradient tree boosting determines the maximum number of \n",
    "features to consider when looking for the best split at each node. Here are some pros \n",
    "and cons of using different options for this parameter:\n",
    "\n",
    "Pros of 'auto':\n",
    "\n",
    "Default value for this parameter\n",
    "Results in a good balance between bias and variance\n",
    "Can help avoid overfitting by limiting the number of features considered\n",
    "Cons of 'auto':\n",
    "\n",
    "May not always select the most informative features\n",
    "May result in a slightly higher bias than other options\n",
    "Pros of 'sqrt':\n",
    "\n",
    "Generally results in a lower bias than 'auto'\n",
    "Limits the number of features considered, helping to avoid overfitting\n",
    "Cons of 'sqrt':\n",
    "\n",
    "May not always select the most informative features\n",
    "May result in a slightly higher variance than 'auto'\n",
    "Pros of 'log2':\n",
    "\n",
    "Similar to 'sqrt', but may result in an even lower bias\n",
    "Limits the number of features considered, helping to avoid overfitting\n",
    "Cons of 'log2':\n",
    "\n",
    "May not always select the most informative features\n",
    "May result in a slightly higher variance than 'auto' or 'sqrt'\n",
    "Overall, the choice of max_features depends on the specific problem and dataset\n",
    "being analyzed. In general, it is a good idea to try out multiple options\n",
    "and see which one performs best through cross-validation.\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Define numerical and categorical column selectors\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "# Get numerical and categorical column names\n",
    "numerical_columns = numerical_columns_selector(X_res)\n",
    "categorical_columns = categorical_columns_selector(X_res)\n",
    "\n",
    "# Define one-hot encoder and scaler\n",
    "onehot = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', onehot, categorical_columns),\n",
    "    ('zscore', scaler, numerical_columns)\n",
    "])\n",
    "\n",
    "# Create a pipeline \n",
    "pipe = Pipeline([\n",
    "    ('make_features', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "# Create search space of hyperparameters\n",
    "search_space = {\n",
    "    'classifier__n_estimators': [50, 100, 150, 200],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 1, 10],\n",
    "    'classifier__max_depth': [3, 5, 7],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create a grid search object to find the best model\n",
    "best_model = GridSearchCV(pipe, search_space, cv=5, verbose=3, n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FITTING OUR BEST MODEL \n",
    "#FINDING THE BEST ACCUARACY \n",
    "#why are we splitting again here? shouldnt we fit on res anyways\n",
    "\n",
    "\n",
    "# Fit grid search\n",
    "\n",
    "%timeit\n",
    "best_model.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS OUR BEST MODEL \n",
    "#IT HAS C = 1, PENALTY = L2, ACCUARACY SCORE OF 67.94%--> improved to 68.6% according to above \n",
    "#USING CROSS VALIDATION \n",
    "\n",
    "\n",
    "# hay our best model has these params, and it turns out its cross validation score was perfect!\n",
    "# note how sklearn does this... once it determines the best version of the classifier\n",
    "# it refits those parameters on ALL of `X_train`... \n",
    "# in other worsds it doesn't just leave the last cross validaiton fold version hanging around :)\n",
    "best_model.best_params_, best_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "###Next is the random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "all the hyper parameters were covered just compare random forest to the other two boosting methods\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Define numerical and categorical column selectors\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "# Get numerical and categorical column names\n",
    "numerical_columns = numerical_columns_selector(X_res)\n",
    "categorical_columns = categorical_columns_selector(X_res)\n",
    "\n",
    "# Define one-hot encoder and scaler\n",
    "onehot = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', onehot, categorical_columns),\n",
    "    ('zscore', scaler, numerical_columns)\n",
    "])\n",
    "\n",
    "# Create a pipeline \n",
    "pipe = Pipeline([\n",
    "    ('make_features', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Create search space of hyperparameters\n",
    "search_space = {\n",
    "    'classifier__n_estimators': [50, 100, 150, 200],\n",
    "    'classifier__criterion': ['gini', 'entropy'],\n",
    "    'classifier__max_depth': [3, 5, 7, None],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create a grid search object to find the best model\n",
    "best_model = GridSearchCV(pipe, search_space, cv=5, verbose=3, n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FITTING OUR BEST MODEL \n",
    "#FINDING THE BEST ACCUARACY \n",
    "#why are we splitting again here? shouldnt we fit on res anyways\n",
    "\n",
    "\n",
    "# Fit grid search\n",
    "%timeit\n",
    "best_model.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS OUR BEST MODEL \n",
    "#IT HAS C = 1, PENALTY = L2, ACCUARACY SCORE OF 67.94%--> improved to 68.6% according to above \n",
    "#USING CROSS VALIDATION \n",
    "\n",
    "\n",
    "# hay our best model has these params, and it turns out its cross validation score was perfect!\n",
    "# note how sklearn does this... once it determines the best version of the classifier\n",
    "# it refits those parameters on ALL of `X_train`... \n",
    "# in other worsds it doesn't just leave the last cross validaiton fold version hanging around :)\n",
    "best_model.best_params_, best_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##I didnt do it cuz i didnt want to interrupt the running code but it would have been ideal to add all these values to a \n",
    "## a list or something for reference but I didnt so just have to look here for the code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO CODE BELOW THIS IS NEEDED - DELETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Wanna make a quick note that we use classification over all the regression method ssince regression is for continuous values\n",
    "Classification works better in our case of approve or not\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "Our proposed solution to predicting whether individuals get approved for a credit card and understanding what variables play a role in making that decision is to use classifiers based on the techniques we’ve learnt in class. \n",
    "First, we will use k-fold validation to determine our model of choice by validating over logistic classifiers/ regression, naive Bayes, Linear SVM, kernel ridge regression with l1, l2, and elastic net penalty. We will choose the model that gives us the highest accuracy score. \n",
    "We are using K-fold validation since with the train-validation-test model split we always run the issue of overfitting on the training data. K-fold validation instead trains and evaluates on all available data by splitting the data amongst each fold training on that and testing on the rest. \n",
    "We are using logistic classifiers/ regression, naive Bayes, Linear SVM, and kernel ridge regression since they are machine learning classifiers. Our proposed problem is to predict whether to approve a credit card for an applicant based on a variety of factors that were asked on their application. This is a yes or no question which makes it a binary classification task. As a result, it makes a lot of sense to use machine learning classification algorithms that can also be used to solve binary classification problems like the algorithms above. We are using these specific algorithms since we are familiar with these algorithms and understand how to run and evaluate them effectively. Furthermore, we use L1, L2, and elasticnet penalty on kernel ridge regression since these are regularization terms that will curb overfitting and make generalization better. \n",
    "With the chosen model we will validate across parameters. In the case of logistic regression, we will validate over the values of C = [0.01, 0.1, 1, 10, 100] to find our best model. \n",
    "For Naive Bayes, we will validate over Gaussian Naive Bayes and Multinomial Naive Bayes and choose our alpha from the following values - [0.01,0.05, 0.1, 0.2, 0.25].\n",
    "For Linear SVM, we will validate over different kernel functions - 'linear', 'poly', 'rbf', and 'sigmoid' and we will choose over values of C = [0.01, 0.1, 1, 10, 100] to find the best model. \n",
    "For Kernel Ridge, we will validate over the following values of alpha - [0.01, 0.1, 1.0, 10.0, 100.0], the kernel functions of linear, polynomial, and RBF and related kernel-specific parameters.\n",
    "Our extensive search over the models and their parameters will make our classification model accurate. As our dataset is not super large, we are not as concerned about computational efficiency - something to improve on while expanding on the project. \n",
    "We will finally train our model and test it to see its accuracy and compare it to existing models available on Kaggle to compare our performance. We will set up a confusion matrix to see how the model fares and plot the ROC/AUC. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "We will use both precision and recall metrics to evaluate our model as both false negatives and false positives are of significant concern to our model. A credit card company would need to maximize eligible customers to increase revenue by minimizing the number of false negatives and also would need to minimize the number of individuals who may default to cut losses hence minimizing the false positives. Since neither one of the metrics is more important to our model, we will additionally use the f1 score which incorporates both precision and recall to finally measure the performance of our model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is extremely suited for this problem since it has a column which is Status which tells based on the previous columns whether or not the credit card request was approved. This was also a dataset of over 10,000 datapoints which is good for our algorithm. We explained the reasoning for our algorithms in another section and our baseline is if its greater than the mean than we accept it. This is our baseline of baselines. Afterwards we will do do k-fold and use that as the baseline to test our hyperparameters and parameters in order to select the best models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next is making all columns which arent numerical numerical\n",
    "#first we will check \"which columns arent numerical\n",
    "credit.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#credit = credit.drop(columns = ['Applicant_Gender', 'Housing_Type', 'Education_Type','Family_Status'])\n",
    "credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the above we see that Applicant_Gender , Housing_Type , Education_Type , Family_Status\n",
    "#check the unique values for applicant_gender\n",
    "credit['Applicant_Gender'].unique()\n",
    "#from below we see that its male or female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#credit['Applicant_Gender'] = credit['Applicant_Gender'] == 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit['Applicant_Gender'] = credit['Applicant_Gender'].replace('M      ',1)\n",
    "credit['Applicant_Gender'] = credit['Applicant_Gender'].replace('F      ',0)\n",
    "credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(x:np.ndarray):\n",
    "    # your code here\n",
    "    uniqueValues = np.unique(x) #so the unique is not the same each time which can cause the names of the columns to be\n",
    "    #incorrect so we will first sort it\n",
    "    numUnique = len(np.unique(x))\n",
    "    #print(sizeOfArrays)\n",
    "    #print(type(uniqueValues))\n",
    "    #oneHot = np.zeros((numUnique, len(x)))\n",
    "    oneHot = np.zeros((len(x), numUnique))\n",
    "    #print(uniqueValues)\n",
    "    i = 0\n",
    "    for val in x:\n",
    "        oneHot[i][np.where(uniqueValues == val)[0][0]] = 1\n",
    "        i += 1\n",
    "    #print(oneHot)\n",
    "    return uniqueValues, oneHot;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#housing one hot encoding\n",
    "housing = credit['Housing_Type']\n",
    "housing = np.array(list(housing))\n",
    "housing\n",
    "print(type(housing))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namesForOneHotHousing, oneHotHousing = one_hot_encode(housing)\n",
    "#oneHotHousing = arrHouse[:]\n",
    "#namesForOneHotHousing = arrHouse[0]\n",
    "print(namesForOneHotHousing)\n",
    "print(oneHotHousing)\n",
    "print(oneHotHousing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(oneHotHousing[:,1])\n",
    "print(sum((oneHotHousing))) #just shows how many in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#namesForOneHotHousing = credit['Housing_Type'].unique()\n",
    "print(namesForOneHotHousing[0])\n",
    "for i in range(oneHotHousing.shape[1]):\n",
    "    credit[namesForOneHotHousing[i]] = oneHotHousing[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay next is education type\n",
    "#education one hot encoding\n",
    "education = credit['Education_Type']\n",
    "education = np.array(list(education))\n",
    "education\n",
    "print(type(education))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namesForOneHotEducation, oneHotEducation = one_hot_encode(education)\n",
    "#oneHotHousing = arrHouse[:]\n",
    "#namesForOneHotHousing = arrHouse[0]\n",
    "print(namesForOneHotEducation)\n",
    "print(oneHotEducation)\n",
    "print(oneHotEducation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(oneHotEducation[:,1])\n",
    "print(sum((oneHotEducation))) #just shows how many in each category\n",
    "print(sum(sum(oneHotEducation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(namesForOneHotEducation[0])\n",
    "for i in range(oneHotEducation.shape[1]):\n",
    "    credit[namesForOneHotEducation[i]] = oneHotEducation[:,i]\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next is family status\n",
    "family = credit['Family_Status']\n",
    "family = np.array(list(family))\n",
    "family\n",
    "print(type(family))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namesForOneHotFamily, oneHotFamily = one_hot_encode(family)\n",
    "#oneHotHousing = arrHouse[:]\n",
    "#namesForOneHotHousing = arrHouse[0]\n",
    "print(namesForOneHotFamily)\n",
    "print(oneHotFamily)\n",
    "print(oneHotFamily.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(oneHotFamily[:,1])\n",
    "print(sum((oneHotFamily))) #just shows how many in each category\n",
    "print(sum(sum(oneHotFamily)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next is Family_Status\n",
    "credit['Family_Status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(namesForOneHotFamily[0])\n",
    "for i in range(oneHotFamily.shape[1]):\n",
    "    credit[namesForOneHotFamily[i]] = oneHotFamily[:,i]\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit['Education_Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit = credit.drop(columns=['Education_Type','Family_Status','Housing_Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit['Applicant_Gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#credit.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #now we are done one hot encoding everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install imblearn\n",
    "#!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(credit.drop('Status', axis=1),\n",
    "                                                    credit['Status'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=123)\n",
    "\n",
    "# Define column transformer for categorical features\n",
    "categorical_features = ['Applicant_Gender', 'Owned_Car', 'Owned_Realty', 'Income_Type',\n",
    "                     'Owned_Mobile_Phone',\n",
    "                        'Owned_Work_Phone', 'Owned_Phone', 'Owned_Email', 'Job_Title']\n",
    "\n",
    "preprocessor = make_column_transformer((OneHotEncoder(handle_unknown='ignore'), categorical_features))\n",
    "\n",
    "# Define ADASYN pipeline with random forest classifier\n",
    "pipeline = make_pipeline(preprocessor, ADASYN(random_state=123), RandomForestClassifier(random_state=123))\n",
    "#pipeline = make_pipeline(ADASYN(random_state=123))\n",
    "\n",
    "# Fit pipeline to training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next is visualization\n",
    "credit.head()\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base Model is  just if the income is better than the mean than approve otherwise don't\n",
    "\n",
    "\n",
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(credit, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_income_mean = train['Total_Income'].mean()\n",
    "print(train_income_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gThanM = [1 if i >= train_income_mean else 0 for i in train['Total_Income']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['prediction'] = gThanM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_num = list(train['Status'])\n",
    "y_pred_num = list(train['prediction'])\n",
    "#print(y_pred_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = 0\n",
    "tp = 0\n",
    "fn = 0\n",
    "tn = 0\n",
    "\n",
    "tpPoints = []\n",
    "xTp = []\n",
    "fpPoints = []\n",
    "xFp = []\n",
    "fnPoints = []\n",
    "xFn = []\n",
    "tnPoints = []\n",
    "xTn = []\n",
    "\n",
    "for i in range (len(y_true_num)):\n",
    "    if y_true_num[i] == 1 and y_pred_num[i] == 1:\n",
    "        tp += 1\n",
    "        tpPoints.append(credit['Total_Income'].iloc[i])\n",
    "        xTp.append(i)\n",
    "        #tpPoints.append(credits['Total_Income'][i]) #fix this doesnt make sense\n",
    "    if y_true_num[i] == 0 and y_pred_num[i] == 1:\n",
    "        fp += 1\n",
    "        fpPoints.append(credit['Total_Income'].iloc[i])\n",
    "        xFp.append(i)\n",
    "        #fpPoints.append(y_true_)\n",
    "    if y_true_num[i] == 1 and y_pred_num[i] == 0:\n",
    "        fn += 1\n",
    "        fnPoints.append(credit['Total_Income'].iloc[i])\n",
    "        xFn.append(i)\n",
    "    if y_true_num[i] == 0 and y_pred_num[i] == 0:\n",
    "        tn += 1\n",
    "        tnPoints.append(credit['Total_Income'].iloc[i])\n",
    "        xTn.append(i)\n",
    "        \n",
    "\n",
    "precision = float (tp) / float(tp + fp)\n",
    "recall = float(tp) / float(tp + fn)\n",
    "        #precision1 = precision_score(y_true_num, y_pred_num)\n",
    "        #print(precision1)\n",
    "        #print(precision)\n",
    "#precise[j] = precision\n",
    "    #j += 1\n",
    "print(\"Precision: \", precision)\n",
    "print()\n",
    "print(\"Recall: \", recall)\n",
    "print()\n",
    "print(\"False Positive: \", fp)\n",
    "print()\n",
    "print(\"False Negative: \", fn)\n",
    "print()\n",
    "f1 = 2*precision *recall / (precision + recall)\n",
    "print(\"F1 Score: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An extrememly low f1 score indicates the baseline isnt great which is what we are aiming for but its a start and what we should beat at the very least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next comes the graph for this\n",
    "plt.scatter(xTp, tpPoints, color = 'green', label=\"True Positive\")\n",
    "plt.scatter(xFp, fpPoints, color = 'yellow', label=\"False Positive\")\n",
    "plt.scatter(xFn, fnPoints, color = 'blue', label = \"False Negatives\")\n",
    "plt.scatter(xTn, tnPoints, color = 'red', label = \"True Negatives\")\n",
    "plt.legend([\"True Positive\" , \"False Positive\", \"False Negative\", \"True Negative\"], ncol = 4 )\n",
    "plt.figure(figsize=(10000,10000))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for our graphs we can use an ROC Curve to evaluate since we are doing a binary classification task\n",
    "#it make sense to compare the tpr to the fpr as the roc curve does\n",
    "\n",
    "\n",
    "#same with doing any of the validation methods like kfold validation or gridsearch their methods only work on methods which\n",
    "#use some sort of classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We referenced the ethics checklist at https://deon.drivendata.org. We have no knowledge of the data collection process since we are using a publicly available dataset at Kaggle, however, we do know that no names are part of the dataset since each applicant was identified by their  ID which protects them. One of the ethical issues with this data is that some information such as the name and date of birth can be identifiable,even though this data set has omitted such private information by using unidentifiable id numbers and age instead of the date of birth.  We haven’t gotten towards the modeling or the deployment therefore we can’t answer the ethics of these two yet. However, basing off of our base model we see that false negatives are a large ethical concern for us as many individuals are considered ineligible by just comparing income. Making our model robust with other economic and credit-worthy factors such as car and home ownership as well as number of family members can help predict individuals who can pay off credit debt more accurately. This ethical corncern will get addressed once a more robust and complex model is in place which takes many factors into account. We will continue to monitor this concern within our model by tracking false positivity rates more closely. Additionally, one confounding variable that could impact our data is race. This variable could possibly be biasing our data, however this variable is not tracked within our dataset hence it is difficult to evaluate whether it is biasing our data and hence biaisng our model. This can be resolved if our model is implemented and trained on data which tracks racial characteristics to further understand the bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *We will communicate multiple times a week through our messages groupchat to collaborate on the project components.*\n",
    "- *We have set aside weekly meetings to work together in person and online via Zoom.* \n",
    "- *We expect to split all components equally and have multiple people working on different sections of the project to have the best output.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/19  |  6 PM |  Brainstorm topics/questions. Everyone brought different project ideas to the meeting.  | Discussed different project ideas; Finalized topic for the project;  discuss hypothesis; began background research | \n",
    "| 2/20  |  N/A |  Found background information. |Presented the background information we found. | \n",
    "| 2/21  | 6 PM  | Edit, finalize, and submit proposal; Search for datasets   | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part; Figured out timeline to work for the project |\n",
    "| 2/28  | 6 PM  | Import & Wrangle Data, do  EDA |Review/Edit wrangling/EDA; Discuss Analysis Plan |\n",
    "| 3/1  | 6 PM  | Finalize wrangling/EDA; Begin programming for project |Discuss/edit project code; Complete project |\n",
    "| 3/13  | NA  | Complete analysis; Draft results/conclusion/discussion | Discuss/edit full project |\n",
    "| 3/22  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Hemkiran, S., et al. “Design of Automatic Credit Card Approval System Using Machine Learning.” SpringerLink, Springer Singapore, 1 Jan. 1970, https://link.springer.com/chapter/10.1007/978-981-16-6448-9_1. <br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Kibria, Golam, and Mehmet Sevkli. “Application of Deep Learning for Credit Card Approval: A Comparison ...” Application of Deep Learning for Credit Card Approval: A Comparison with Two Machine Learning Techniques, https://www.researchgate.net/profile/Md-Kibria-12/publication/348755769_Application_of_Deep_Learning_for_Credit_Card_Approval_A_Comparison_with_Two_Machine_Learning_Techniques/links/600f495f92851c13fe39bb38/Application-of-Deep-Learning-for-Credit-Card-Approval-A-Comparison-with-Two-Machine-Learning-Techniques.pdf?origin=publication_detail. <br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota):Markova, Maya. “Credit Card Approval Model: An Application of Deep Neural Networks.” AIP Publishing, AIP Publishing LLC AIP Publishing, 24 Feb. 2021, https://aip.scitation.org/doi/abs/10.1063/5.0040744?journalCode=apc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
