{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Group members\n",
    "\n",
    "- Neil Bajaj\n",
    "- Ria Singh\n",
    "- Pratheek Sankeshi \n",
    "- Shenova Davis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "The goal of this project is to predict whether to approve a credit card for an applicant based on a variety of factors that were asked on their application. We will predict using a machine learning algorithm. The data we will be using is from the Kaggle dataset: A Credit Card Dataset for Machine Learning. The link is https://www.kaggle.com/datasets/caesarmario/application-data/. We will be using factors such as Total Income, Education Type, Applicant Age, etc. More will be described in the data section. Furthermore, we will drop unnecessary columns like Owned Phone, Owned Email, etc while also replacing null values to perform EDA. We will then run various supervised machine learning algorithms to create models to predict the data and use the best one. The performance will be measured on how accurately we predict the data against the status column of the data which is whether the application was approved or not. \n",
    "\n",
    "\n",
    "__NB:__ this final project form is much more report-like than the proposal and the checkpoint. Think in terms of writing a paper with bits of code in the middle to make the plots/tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "As we get deeper into the age of Big Data, we see that, where possible, we are attempting to move from human evaluation to machine learning prediction. There has also been an uptick in the number of credit card defaulters. Using this logic, credit card companies need a new way to decide whether or not to approve credit cards based on prior history. \n",
    "\n",
    "A previous study done by Dr. Hemkiran[1] evaluated whether applicants should be approved for a credit card by using a Logistic Regression with and without a grid search technique. They found that using a grid search technique improved the competency of their model. Additionally, they also used an Artificial Neural Network (ANN) and found it to be better than the linear regression model. Another study by Dr. Kibria[2] aimed to create a deep learning model to aid credit card approval decision-making. They also used a logistic regression model and a support vector machine (SVM) model to compare their results. They found that the deep learning model was better than the logistic regression and SVM models. However, the ANN model and deep learning model are more computationally expensive and time-consuming. \n",
    "\n",
    "We are attempting to create a predictive machine learning model that models whether or not the application has the right credentials to have their credit card application approved using the following variables: Applicant Gender, Owned property, Total Children, Owned Car, Total Income, Housing Type, Total Family Members, Applicant Age, Education Type, and Family Status. This is to avoid future credit card defaulters. We will use models such as Naive Bayes Classifier, Linear SVM, and Linear Regression and use them to compare our modelâ€™s performance as well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "We are building this model to measure if there are any discrepancies when approving credit card applications. We additionally want to check if we could use big data and machine learning to build a model that predicts if a credit card is approved or not. This would take out human bias from the equation and make sure every application is fairly reviewed. Creating a machine learning model will attempt to eliminate the human bias towards race, class, gender, etc - an issue that plagues the financial ratings of individuals. Additionally, a machine-learning system can significantly reduce the human power and costs of a credit card company, increasing revenue. If our model can accurately predict whether an application can be accepted, we could conclude our hypothesis. This problem is quantifiable since we are trying to model a binary predictor. This problem is measurable because we would be using the metric of accuracy to validate the performance of our model. Lastly, our model is replicable because we could run it on different datasets and check its accuracy on each dataset. The model we will create will be composed of supervised machine-learning algorithms and techniques such as logistic regression, linear SVM, K fold validation, etc. We will train our model on previously collected data from credit card companies to understand what attributes make an individual more or less likely to get approved for a credit card. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "- Link for our data: https://www.kaggle.com/datasets/caesarmario/application-data\n",
    "\n",
    "- This data set has about 25,100 observations with 21 variables. \n",
    "\n",
    "- The variables that will be used are Applicant Gender, Owned property, Total Children, Owned Car, Total Income, Housing Type, Total Family Members, Applicant Age, Education Type,Family Status, Total Good Debt, and Total Bad Debt. \n",
    "\n",
    "- The dataset we are using has already been cleaned to drop any data points with null values and yet the dataset remains robust so we will not be addressing it any further.\n",
    "\n",
    "- We will additionally drop all the features that we will not be using to further declutter the dataset.\n",
    "\n",
    "- In addition to cleaning we one hot encoded all other categorical data which wasn't already binary which is displayed below\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns    \n",
    "from matplotlib.pyplot import figure\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "#models importing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "\n",
    "#linear svm stuff\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "credit  = pd.read_csv('Application_Data.csv')\n",
    "\n",
    "#Keeping only the columns we intend on using along with the status \n",
    "#column which contains the true status of the applicant\n",
    "\n",
    "credit = credit[[\"Applicant_Gender\", \"Applicant_Age\",\"Owned_Realty\",\n",
    "                 \"Total_Children\", \"Owned_Car\", \"Total_Income\", \"Housing_Type\",\n",
    "                 \"Total_Family_Members\", \"Education_Type\", \"Family_Status\", 'Status', 'Total_Bad_Debt', 'Total_Good_Debt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Applicant_Gender</th>\n",
       "      <th>Applicant_Age</th>\n",
       "      <th>Owned_Realty</th>\n",
       "      <th>Total_Children</th>\n",
       "      <th>Owned_Car</th>\n",
       "      <th>Total_Income</th>\n",
       "      <th>Housing_Type</th>\n",
       "      <th>Total_Family_Members</th>\n",
       "      <th>Education_Type</th>\n",
       "      <th>Family_Status</th>\n",
       "      <th>Status</th>\n",
       "      <th>Total_Bad_Debt</th>\n",
       "      <th>Total_Good_Debt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112500</td>\n",
       "      <td>House / apartment                             ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Secondary / secondary special                 ...</td>\n",
       "      <td>Married                                       ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>270000</td>\n",
       "      <td>House / apartment                             ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary / secondary special                 ...</td>\n",
       "      <td>Single / not married                          ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>270000</td>\n",
       "      <td>House / apartment                             ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary / secondary special                 ...</td>\n",
       "      <td>Single / not married                          ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>270000</td>\n",
       "      <td>House / apartment                             ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary / secondary special                 ...</td>\n",
       "      <td>Single / not married                          ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>270000</td>\n",
       "      <td>House / apartment                             ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary / secondary special                 ...</td>\n",
       "      <td>Single / not married                          ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Applicant_Gender  Applicant_Age  Owned_Realty  Total_Children  Owned_Car  \\\n",
       "0          M                   59             1               0          1   \n",
       "1          F                   53             1               0          0   \n",
       "2          F                   53             1               0          0   \n",
       "3          F                   53             1               0          0   \n",
       "4          F                   53             1               0          0   \n",
       "\n",
       "   Total_Income                                       Housing_Type  \\\n",
       "0        112500  House / apartment                             ...   \n",
       "1        270000  House / apartment                             ...   \n",
       "2        270000  House / apartment                             ...   \n",
       "3        270000  House / apartment                             ...   \n",
       "4        270000  House / apartment                             ...   \n",
       "\n",
       "   Total_Family_Members                                     Education_Type  \\\n",
       "0                     2  Secondary / secondary special                 ...   \n",
       "1                     1  Secondary / secondary special                 ...   \n",
       "2                     1  Secondary / secondary special                 ...   \n",
       "3                     1  Secondary / secondary special                 ...   \n",
       "4                     1  Secondary / secondary special                 ...   \n",
       "\n",
       "                                       Family_Status  Status  Total_Bad_Debt  \\\n",
       "0  Married                                       ...       1               0   \n",
       "1  Single / not married                          ...       1               0   \n",
       "2  Single / not married                          ...       1               0   \n",
       "3  Single / not married                          ...       1               0   \n",
       "4  Single / not married                          ...       1               0   \n",
       "\n",
       "   Total_Good_Debt  \n",
       "0               30  \n",
       "1                5  \n",
       "2                5  \n",
       "3               27  \n",
       "4               39  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "While doing the initial EDA, we realised that our dataset only had 0.4% negative class. To rectify this discrepancy, we used SMOTE to generate data such that the data is split at an even 50% - similar to actual credit card approval rates. However, before running SMOTE we need to One Hot Encode our data.\n",
    "\n",
    "\n",
    "The data generation results in the total number of data points increasing from 25,000 to 40,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before One Hot Encoding, we check for null or NAN values - as seen below our dataset doesn't have any hence handling them is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(credit.drop('Status', axis=1), #\n",
    "                                                    credit['Status'], #Y variable target\n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=123)\n",
    "\n",
    "categorical_features = [\"Applicant_Gender\", \"Housing_Type\", \"Education_Type\", \"Family_Status\"]\n",
    "\n",
    "\n",
    "# Define numerical and categorical column selectors\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "# Get numerical and categorical column names\n",
    "numerical_columns = numerical_columns_selector(X_train)\n",
    "categorical_columns = categorical_columns_selector(X_train)\n",
    "\n",
    "# Define one-hot encoder and scaler\n",
    "numerical_transformer = StandardScaler()\n",
    "\n",
    "categorical_transformer = OneHotEncoder(drop='first')  # drop original categorical features, could combine with if binary\n",
    "#but unnecessary\n",
    "\n",
    "#creating a preprocessor to add to the pipeline maybe combine this cell into one?\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[ \n",
    "        ('num', numerical_transformer, numerical_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing some additional data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.1505612  -1.38262266 -0.6654445  ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.62492912 -1.38262266  0.64664626 ...  0.          0.\n",
      "   0.        ]\n",
      " [-1.04269155  0.72326313  0.64664626 ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [-1.45864057  0.72326313 -0.6654445  ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.76180089  0.72326313 -0.6654445  ...  0.          0.\n",
      "   0.        ]\n",
      " [-1.01202952  0.72326313 -0.6654445  ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_columns = list(numerical_columns)\n",
    "\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "#cat_columns = list(categorical_transformer.get_feature_names_out(categorical_columns))\n",
    "cat_columns = list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_columns))\n",
    "column_names = num_columns + cat_columns\n",
    "column_names = num_columns + cat_columns\n",
    "\n",
    "X_res_df = pd.DataFrame(X_res, columns=column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    40004.000000\n",
       "mean        -0.041293\n",
       "std          1.006789\n",
       "min         -1.630637\n",
       "25%         -0.660573\n",
       "50%         -0.315613\n",
       "75%          0.299036\n",
       "max         13.455899\n",
       "Name: Total_Income, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_res_df['Total_Income'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2.010200e+04\n",
       "mean     1.943165e+05\n",
       "std      1.026106e+05\n",
       "min      2.700000e+04\n",
       "25%      1.350000e+05\n",
       "50%      1.800000e+05\n",
       "75%      2.250000e+05\n",
       "max      1.575000e+06\n",
       "Name: Total_Income, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['Total_Income'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# @SHENOVA - REWRITE PLS\n",
    "\n",
    "The reason why they look different and transformed is due to standard scaling \n",
    "Standard scaling (also known as standardization) has several benefits:\n",
    "\n",
    "Normalization of data: Standard scaling transforms the data to have zero mean and unit variance, which helps in normalizing the data. This is particularly useful when the data has different units of measurement or scales.\n",
    "\n",
    "Better performance of some machine learning algorithms: Some machine learning algorithms like K-nearest neighbors (KNN) and SVM (support vector machines) are sensitive to the scale of the input features. Standard scaling can improve the performance of these algorithms.\n",
    "\n",
    "Efficient optimization: Many optimization algorithms like gradient descent converge faster when the input features are on the same scale.\n",
    "\n",
    "Interpretation of coefficients: When performing linear regression or other models with coefficients, standard scaling ensures that the coefficients can be compared fairly, as they are on the same scale.\n",
    "\n",
    "Overall, standard scaling is a common preprocessing step that can improve the performance of many machine learning models and make the interpretation of results easier.\n",
    "\n",
    "\n",
    "Due to the reasons above, I believe we should keep standard scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our data cleaning process and make the data ready to be used in our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "Our proposed solution to predicting whether individuals get approved for a credit card and understanding what variables play a role in making that decision is to use classifiers based on the techniques weâ€™ve learnt in class. \n",
    "First, we will use k-fold validation to determine our model of choice by validating over logistic classifiers/ regression, naive Bayes, Linear SVM, kernel ridge regression with l1, l2, and elastic net penalty. We will choose the model that gives us the highest accuracy score. \n",
    "\n",
    "We are using K-fold validation since with the train-validation-test model split we always run the issue of overfitting on the training data. K-fold validation instead trains and evaluates on all available data by splitting the data amongst each fold training on that and testing on the rest. \n",
    "\n",
    "We are using logistic classifiers/ regression, naive Bayes, Linear SVM, and kernel ridge regression since they are machine learning classifiers. Our proposed problem is to predict whether to approve a credit card for an applicant based on a variety of factors that were asked on their application. This is a yes or no question which makes it a binary classification task. As a result, it makes a lot of sense to use machine learning classification algorithms that can also be used to solve binary classification problems like the algorithms above. We are using these specific algorithms since we are familiar with these algorithms and understand how to run and evaluate them effectively. Furthermore, we use L1, L2, and elasticnet penalty on kernel ridge regression since these are regularization terms that will curb overfitting and make generalization better. \n",
    "\n",
    "With the chosen model we will validate across parameters. In the case of logistic regression, we will validate over the values of C = [0.01, 0.1, 1, 10, 100] to find our best model. \n",
    "For Naive Bayes, we will validate over Gaussian Naive Bayes and Multinomial Naive Bayes and choose our alpha from the following values - [0.01,0.05, 0.1, 0.2, 0.25].\n",
    "\n",
    "For Linear SVM, we will validate over different kernel functions - 'linear', 'poly', 'rbf', and 'sigmoid' and we will choose over values of C = [0.01, 0.1, 1, 10, 100] to find the best model. \n",
    "\n",
    "For Kernel Ridge, we will validate over the following values of alpha - [0.01, 0.1, 1.0, 10.0, 100.0], the kernel functions of linear, polynomial, and RBF and related kernel-specific parameters.\n",
    "\n",
    "Our extensive search over the models and their parameters will make our classification model accurate. As our dataset is not super large, we are not as concerned about computational efficiency - something to improve on while expanding on the project. \n",
    "\n",
    "We will finally train our model and test it to see its accuracy and compare it to existing models available on Kaggle to compare our performance. We will set up a confusion matrix to see how the model fares and plot the ROC/AUC. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "We will use both precision and recall metrics to evaluate our model as both false negatives and false positives are of significant concern to our model. A credit card company would need to maximize eligible customers to increase revenue by minimizing the number of false negatives and also would need to minimize the number of individuals who may default to cut losses hence minimizing the false positives. Since neither one of the metrics is more important to our model, we will additionally use the f1 score which incorporates both precision and recall to finally measure the performance of our model. \n",
    "\n",
    "## Add picture here instead of formulas!!\n",
    "Precision is defined as Correctly Classified Positives / Predicted Positives. It helps gauge false positivity rates\n",
    "\n",
    "Recall is defined as Correctly Classfied Positives/ All True Positives.\n",
    "\n",
    "F1 score is 2(Correctly Classified Positives)/2(Correctly Classified Positives) + False Positive + False Negative\n",
    "\n",
    "Accuracy is defined as Correctly Classified data/ All data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "After cleaning and balancing our dataset,we proceed to train and validate our data on the training set using reapeated KFold crossvalidation where K =5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the baseline model we created where applicants with income higher than median income of the dataset get approved for the credit card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194316.49268729478\n",
      "Precision:  0.9950829748002459\n",
      "\n",
      "Recall:  0.4047095290470953\n",
      "\n",
      "False Positive:  40\n",
      "\n",
      "False Negative:  11907\n",
      "\n",
      "F1 Score:  0.5753989408963287\n"
     ]
    }
   ],
   "source": [
    "train_income_mean = X_train['Total_Income'].mean()\n",
    "print(train_income_mean)\n",
    "\n",
    "Baseline = [1 if i >= train_income_mean else 0 for i in X_train['Total_Income']]\n",
    "X_train['prediction'] = Baseline\n",
    "\n",
    "\n",
    "y_true_num = list(y_train)\n",
    "y_pred_num = list(X_train['prediction'])\n",
    "\n",
    "fp = 0\n",
    "tp = 0\n",
    "fn = 0\n",
    "tn = 0\n",
    "\n",
    "tpPoints = []\n",
    "xTp = []\n",
    "fpPoints = []\n",
    "xFp = []\n",
    "fnPoints = []\n",
    "xFn = []\n",
    "tnPoints = []\n",
    "xTn = []\n",
    "\n",
    "for i in range (len(y_true_num)):\n",
    "    if y_true_num[i] == 1 and y_pred_num[i] == 1:\n",
    "        tp += 1\n",
    "        tpPoints.append(credit['Total_Income'].iloc[i])\n",
    "        xTp.append(i)\n",
    "        #tpPoints.append(credits['Total_Income'][i]) #fix this doesnt make sense\n",
    "    if y_true_num[i] == 0 and y_pred_num[i] == 1:\n",
    "        fp += 1\n",
    "        fpPoints.append(credit['Total_Income'].iloc[i])\n",
    "        xFp.append(i)\n",
    "        #fpPoints.append(y_true_)\n",
    "    if y_true_num[i] == 1 and y_pred_num[i] == 0:\n",
    "        fn += 1\n",
    "        fnPoints.append(credit['Total_Income'].iloc[i])\n",
    "        xFn.append(i)\n",
    "    if y_true_num[i] == 0 and y_pred_num[i] == 0:\n",
    "        tn += 1\n",
    "        tnPoints.append(credit['Total_Income'].iloc[i])\n",
    "        xTn.append(i)\n",
    "        \n",
    "\n",
    "precision = float (tp) / float(tp + fp)\n",
    "recall = float(tp) / float(tp + fn)\n",
    "       \n",
    "print(\"Precision: \", precision)\n",
    "print()\n",
    "print(\"Recall: \", recall)\n",
    "print()\n",
    "print(\"False Positive: \", fp)\n",
    "print()\n",
    "print(\"False Negative: \", fn)\n",
    "print()\n",
    "f1 = 2*precision *recall / (precision + recall)\n",
    "print(\"F1 Score: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier 1 - Logistic Classifier\n",
    "Include code and accuracy - only include the relevant hyperparameters that converge and work\n",
    "\n",
    "make verbose (GridSearch) = 2/1 not 3\n",
    "\n",
    "write up - include why the model, pros and cons, how it performs and which hyperparametrs perform best according to our comparison metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding our best model \n",
    "\n",
    "Here are the benefits and cons of each solver:\n",
    "\n",
    "'saga': This solver supports both L1 and L2 regularization and is capable of handling large datasets. It is recommended when the number of samples is significantly larger than the number of features. However, it may be slower than other solvers when the number of features is large.\n",
    "\n",
    "'lbfgs': This solver is recommended when the dataset is small or medium-sized. It is a limited-memory quasi-Newton method that uses a line search algorithm. It can handle both L1 and L2 regularization, but it may not be suitable for large datasets.\n",
    "\n",
    "'newton-cg': This solver is a quasi-Newton method that uses a Hessian matrix approximation. It can handle both L1 and L2 regularization, but it may not be suitable for large datasets.\n",
    "\n",
    "'sag': This solver is a stochastic gradient descent (SGD) algorithm. It is recommended when the dataset is large and cannot fit into memory. It is fast and can handle both L1 and L2 regularization. However, it may not converge as fast as other solvers.\n",
    "\n",
    "'newton-cholesky': This solver is a quasi-Newton method that uses a Cholesky decomposition to compute the inverse of the Hessian matrix. It can handle both L1 and L2 regularization, but it may not be suitable for large datasets.\n",
    "\n",
    "'liblinear': This solver is a linear algorithm that uses a coordinate descent method. It is recommended when the dataset is small or medium-sized and the number of samples is greater than the number of features. It can only handle L1 regularization.\n",
    "\n",
    "\n",
    "C values: smaller c-value is equal to greater generalization (simpler model) and stronger regularization\n",
    "but maybe too much while larger could mean greater overfitting (more complex model) and weaker regularization\n",
    "\n",
    "# @Shenova\n",
    "The two penalties I will leave to u to write about:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(X_res)\n",
    "categorical_columns = categorical_columns_selector(X_res)\n",
    "\n",
    "onehot = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', onehot, categorical_columns),\n",
    "    ('zscore', scaler, numerical_columns)])\n",
    "\n",
    "\n",
    "# Create a pipeline \n",
    "pipe = Pipeline([('make_features', preprocessor),\n",
    "                 ('classifier', LogisticRegression(max_iter=10000))])\n",
    "\n",
    "search_space = {'classifier__C': np.logspace(-4, 4, 9),\n",
    "               'classifier__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "               'classifier__solver': ['saga', 'lbfgs', 'newton-cg', 'sag', 'newton-cholesky', 'liblinear']}\n",
    "\n",
    "# Create a grid search object to find the best model\n",
    "best_model = GridSearchCV(pipe, search_space, cv=5, verbose=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit grid search\n",
    "best_model.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### possible overfit as results are showing 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.best_params_, best_model.best_score_ #maybe too good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Best model is Solver = 'lbfgs',  C = 1000.0,  PENALTY = l2, and accuracy of 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier 2 - SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "We also need to explain why we choose svc rather than svm's other classifiers in sklearn\n",
    "\n",
    "\n",
    "'classifier__C': np.logspace(-4, 4, 9), <- just the usual C value stuff\n",
    "               'classifier__kernel': ['linear', 'rbf', 'sigmoid', 'poly'], <-\n",
    "        'linear': This kernel is a linear transformation that maps the data to a higher-dimensional \n",
    "        space using a linear function. It is suitable for linearly separable datasets and works well \n",
    "        when there are many features. The 'linear' kernel is computationally efficient and is less prone\n",
    "        to overfitting than other kernels. However, it may not work well on nonlinear datasets.\n",
    "\n",
    "        'rbf' (Radial basis function): This kernel is a popular choice for nonlinear datasets. \n",
    "        It maps the data to a high-dimensional feature space using a Gaussian function. \n",
    "        It can capture complex relationships between features and is very flexible. \n",
    "        However, the 'rbf' kernel can be sensitive to the choice of hyperparameters, such as the \n",
    "        width of the Gaussian function, and can overfit when the number of features is large.\n",
    "\n",
    "        'sigmoid': This kernel maps the data to a high-dimensional feature space using a sigmoid \n",
    "        function. It is suitable for problems that require a logistic function as a decision function.\n",
    "        However, the 'sigmoid' kernel is less popular than the other kernels and may not perform as \n",
    "        well on many datasets.\n",
    "\n",
    "        'poly' (Polynomial): This kernel maps the data to a high-dimensional feature space using \n",
    "        a polynomial function. It is suitable for datasets with nonlinear relationships between \n",
    "        features. However, the 'poly' kernel can be sensitive to the degree of the polynomial \n",
    "        and can overfit when the degree is too high. <- was the best before i had to refit the data not sure whats happening\n",
    "        rn\n",
    "               \n",
    "               \n",
    "               \n",
    "               \n",
    "               \n",
    "            'classifier__gamma': ['scale', 'auto', 0.1, 1, 10] <-Kernel coefficient for â€˜rbfâ€™, â€˜polyâ€™ and â€˜sigmoidâ€™.\n",
    "\n",
    "if gamma='scale' (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma,\n",
    "\n",
    "if â€˜autoâ€™, uses 1 / n_features\n",
    "\n",
    "if float, must be non-negative.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(X_res)\n",
    "categorical_columns = categorical_columns_selector(X_res)\n",
    "\n",
    "onehot = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', onehot, categorical_columns),\n",
    "    ('zscore', scaler, numerical_columns)])\n",
    "\n",
    "\n",
    "# Create a pipeline \n",
    "pipe = Pipeline([('make_features', preprocessor),\n",
    "                 ('classifier', SVC())])\n",
    "\n",
    "# Create search space of hyperparameters\n",
    "# the variable namespace looks like this\n",
    "# pipe.classifier.C is represented as 'classifier__C'\n",
    "# if we'd just chucked a LogisticRegression() in as the model\n",
    "# instead of a pipe, then we'd only have had 'C' w/o the 'classifier__' bit \n",
    "search_space = {'classifier__C': np.logspace(-4, 4, 9),\n",
    "               'classifier__kernel': ['linear', 'rbf', 'sigmoid', 'poly'],\n",
    "               'classifier__gamma': ['scale', 'auto', 0.1, 1, 10]}\n",
    "\n",
    "# Create a grid search object to find the best model\n",
    "best_model = GridSearchCV(pipe, search_space, cv=5, verbose=3, n_jobs=-1) #idk why the verbose isnt showing for me anymore\n",
    "#it just runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FITTING OUR BEST MODEL \n",
    "#FINDING THE BEST ACCUARACY \n",
    "\n",
    "best_model.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.best_params_, best_model.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best model is ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier 3 - Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive bayes has no hyperparameter so just explain naive bayes itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Define numerical and categorical column selectors\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "# Get numerical and categorical column names\n",
    "numerical_columns = numerical_columns_selector(X_res)\n",
    "categorical_columns = categorical_columns_selector(X_res)\n",
    "\n",
    "# Define one-hot encoder and scaler\n",
    "onehot = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', onehot, categorical_columns),\n",
    "    ('zscore', scaler, numerical_columns)\n",
    "])\n",
    "\n",
    "# Create a pipeline \n",
    "pipe = Pipeline([\n",
    "    ('make_features', preprocessor),\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "# Create search space of hyperparameters\n",
    "search_space = {}\n",
    "\n",
    "# Create a grid search object to find the best model\n",
    "best_model = GridSearchCV(pipe, search_space, cv=5, verbose=3, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit grid search\n",
    "best_model.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.best_params_, best_model.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best model is ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier 4 - KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the weights are quite literally that uniform is just 1/n and distance values the closer the neighbors \n",
    "the more value it willhave\n",
    "\n",
    "finally the algorithms: \n",
    "'auto': This is the default algorithm, which selects the most appropriate algorithm \n",
    "based on the characteristics of the data. It is a good choice for most datasets.\n",
    "\n",
    "Pros:\n",
    "\n",
    "Automatic selection of the best algorithm based on the input data.\n",
    "Suitable for most datasets.\n",
    "Cons:\n",
    "\n",
    "None.\n",
    "\n",
    "\n",
    "'ball_tree': This algorithm constructs a ball tree data structure to find the nearest \n",
    "neighbors. It is a good choice when the number of features is large compared to the number of samples. #not good for ours\n",
    "\n",
    "Pros:\n",
    "\n",
    "Efficient for high-dimensional datasets.\n",
    "Can be faster than brute force algorithm for some datasets.\n",
    "Cons:\n",
    "\n",
    "Slower than brute force algorithm for low-dimensional datasets. Memory-intensive.\n",
    "\n",
    "\n",
    "'kd_tree': This algorithm constructs a kd-tree data structure to find the nearest neighbors. \n",
    "It is a good choice when the number of features is small compared to the number of samples. #good for ours\n",
    "\n",
    "Pros:\n",
    "\n",
    "Efficient for low-dimensional datasets.\n",
    "Can be faster than brute force algorithm for some datasets.\n",
    "Cons:\n",
    "\n",
    "Slower than brute force algorithm for high-dimensional datasets.\n",
    "Memory-intensive.\n",
    "\n",
    "\n",
    "'brute': This algorithm computes the distances between all pairs of points in the dataset to find the nearest\n",
    "neighbors. It is a good choice for small datasets. #ok for ours i think kd-tree is better\n",
    "\n",
    "Pros:\n",
    "\n",
    "Simple and easy to understand.\n",
    "Suitable for small datasets.\n",
    "Cons:\n",
    "\n",
    "Slow and inefficient for large datasets. Memory-intensive.\n",
    "\n",
    "In general, the choice of algorithm depends on the size and characteristics of the dataset. \n",
    "If the number of features is large, ball tree algorithm may be a good choice, whereas if the number of samples is small and the number of features is low, kd-tree algorithm may be a good choice. If the dataset is small, brute force algorithm may be a good choice. The auto option can be a good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Define numerical and categorical column selectors\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "# Get numerical and categorical column names\n",
    "numerical_columns = numerical_columns_selector(X_res)\n",
    "categorical_columns = categorical_columns_selector(X_res)\n",
    "\n",
    "# Define one-hot encoder and scaler\n",
    "onehot = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', onehot, categorical_columns),\n",
    "    ('zscore', scaler, numerical_columns)\n",
    "])\n",
    "\n",
    "# Create a pipeline \n",
    "pipe = Pipeline([\n",
    "    ('make_features', preprocessor),\n",
    "    ('classifier', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Create search space of hyperparameters\n",
    "search_space = {\n",
    "    'classifier__n_neighbors': range(1, 31),\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "}\n",
    "\n",
    "# Create a grid search object to find the best model\n",
    "best_model = GridSearchCV(pipe, search_space, cv=5, verbose=3, n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.best_params_, best_model.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Our best model is ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier 5 - Kernel Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We covered the kernel values here \n",
    "\n",
    "the gamma value  determines the inverse of the width of the kernel function, \n",
    "which in turn affects the degree of smoothing of the resulting regression function.\n",
    "\n",
    "The 'scale' value of the regressor__gamma hyperparameter specifies \n",
    "that gamma should be set to 1 / (n_features * X.var()), where n_features is \n",
    "the number of features in the input data X. This is equivalent to scaling each feature \n",
    "to have unit variance before computing gamma.\n",
    "\n",
    "The 'auto' value of regressor__gamma specifies that gamma should be set to 1 / n_features. \n",
    "This is equivalent to using the median of the distances between all pairs of points in the \n",
    "input data X to set the scale of the kernel function.\n",
    "\n",
    "Finally the alpha value a lower value like 0.001 means weak regularization and more complex model\n",
    "with greater overfittign and vice versa for values like a 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Define numerical and categorical column selectors\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "# Get numerical and categorical column names\n",
    "numerical_columns = numerical_columns_selector(X_res)\n",
    "categorical_columns = categorical_columns_selector(X_res)\n",
    "\n",
    "# Define one-hot encoder and scaler\n",
    "onehot = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', onehot, categorical_columns),\n",
    "    ('zscore', scaler, numerical_columns)\n",
    "])\n",
    "\n",
    "# Create a pipeline \n",
    "pipe = Pipeline([\n",
    "    ('make_features', preprocessor),\n",
    "    ('regressor', KernelRidge())\n",
    "])\n",
    "\n",
    "# Create search space of hyperparameters\n",
    "search_space = {\n",
    "    'regressor__alpha': [0.1, 1, 10],\n",
    "    'regressor__kernel': ['linear', 'rbf', 'polynomial'],\n",
    "    'regressor__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Create a grid search object to find the best model\n",
    "best_model = GridSearchCV(pipe, search_space, cv=5, verbose=3, n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FITTING OUR BEST MODEL \n",
    "#FINDING THE BEST ACCUARACY \n",
    "#why are we splitting again here? shouldnt we fit on res anyways\n",
    "\n",
    "\n",
    "# Fit grid search\n",
    "\n",
    "best_model.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model.best_params_, best_model.best_score_\n",
    "\n",
    "#this method doesnt run well since it takes up too much ram about 7.5 gb of ram and thus isnt possible to do with our systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier 6 - Ensemble(AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Define numerical and categorical column selectors\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "# Get numerical and categorical column names\n",
    "numerical_columns = numerical_columns_selector(X_res)\n",
    "categorical_columns = categorical_columns_selector(X_res)\n",
    "\n",
    "# Define one-hot encoder and scaler\n",
    "onehot = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', onehot, categorical_columns),\n",
    "    ('zscore', scaler, numerical_columns)\n",
    "])\n",
    "\n",
    "# Create a pipeline \n",
    "pipe = Pipeline([\n",
    "    ('make_features', preprocessor),\n",
    "    ('classifier', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "# Create search space of hyperparameters\n",
    "search_space = {\n",
    "    'classifier__n_estimators': [50, 100, 150, 200],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 1, 10],\n",
    "    'classifier__algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "\n",
    "# Create a grid search object to find the best model\n",
    "best_model = GridSearchCV(pipe, search_space, cv=5, verbose=3, n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "#FITTING OUR BEST MODEL \n",
    "#FINDING THE BEST ACCUARACY \n",
    "#why are we splitting again here? shouldnt we fit on res anyways\n",
    "\n",
    "\n",
    "# Fit grid search\n",
    "best_model.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.best_params_, best_model.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier 7 - Ensemble(Gradient tree boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "estimators and learning rate i covered above\n",
    "and since its a tree we know what the depth, leaves, and split  means as well\n",
    "only the max features are left:\n",
    "\n",
    "The max_features parameter in gradient tree boosting determines the maximum number of \n",
    "features to consider when looking for the best split at each node. Here are some pros \n",
    "and cons of using different options for this parameter:\n",
    "\n",
    "Pros of 'auto':\n",
    "\n",
    "Default value for this parameter\n",
    "Results in a good balance between bias and variance\n",
    "Can help avoid overfitting by limiting the number of features considered\n",
    "Cons of 'auto':\n",
    "\n",
    "May not always select the most informative features\n",
    "May result in a slightly higher bias than other options\n",
    "Pros of 'sqrt':\n",
    "\n",
    "Generally results in a lower bias than 'auto'\n",
    "Limits the number of features considered, helping to avoid overfitting\n",
    "Cons of 'sqrt':\n",
    "\n",
    "May not always select the most informative features\n",
    "May result in a slightly higher variance than 'auto'\n",
    "Pros of 'log2':\n",
    "\n",
    "Similar to 'sqrt', but may result in an even lower bias\n",
    "Limits the number of features considered, helping to avoid overfitting\n",
    "Cons of 'log2':\n",
    "\n",
    "May not always select the most informative features\n",
    "May result in a slightly higher variance than 'auto' or 'sqrt'\n",
    "Overall, the choice of max_features depends on the specific problem and dataset\n",
    "being analyzed. In general, it is a good idea to try out multiple options\n",
    "and see which one performs best through cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Define numerical and categorical column selectors\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "# Get numerical and categorical column names\n",
    "numerical_columns = numerical_columns_selector(X_res)\n",
    "categorical_columns = categorical_columns_selector(X_res)\n",
    "\n",
    "# Define one-hot encoder and scaler\n",
    "onehot = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', onehot, categorical_columns),\n",
    "    ('zscore', scaler, numerical_columns)\n",
    "])\n",
    "\n",
    "# Create a pipeline \n",
    "pipe = Pipeline([\n",
    "    ('make_features', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "# Create search space of hyperparameters\n",
    "search_space = {\n",
    "    'classifier__n_estimators': [50, 100, 150, 200],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 1, 10],\n",
    "    'classifier__max_depth': [3, 5, 7],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create a grid search object to find the best model\n",
    "best_model = GridSearchCV(pipe, search_space, cv=5, verbose=3, n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "#FITTING OUR BEST MODEL \n",
    "#FINDING THE BEST ACCUARACY \n",
    "#why are we splitting again here? shouldnt we fit on res anyways\n",
    "\n",
    "\n",
    "# Fit grid search\n",
    "\n",
    "best_model.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.best_params_, best_model.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier 8 - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rom sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Define numerical and categorical column selectors\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "# Get numerical and categorical column names\n",
    "numerical_columns = numerical_columns_selector(X_res)\n",
    "categorical_columns = categorical_columns_selector(X_res)\n",
    "\n",
    "# Define one-hot encoder and scaler\n",
    "onehot = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', onehot, categorical_columns),\n",
    "    ('zscore', scaler, numerical_columns)\n",
    "])\n",
    "\n",
    "# Create a pipeline \n",
    "pipe = Pipeline([\n",
    "    ('make_features', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Create search space of hyperparameters\n",
    "search_space = {\n",
    "    'classifier__n_estimators': [50, 100, 150, 200],\n",
    "    'classifier__criterion': ['gini', 'entropy'],\n",
    "    'classifier__max_depth': [3, 5, 7, None],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create a grid search object to find the best model\n",
    "best_model = GridSearchCV(pipe, search_space, cv=5, verbose=3, n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FITTING OUR BEST MODEL \n",
    "#FINDING THE BEST ACCUARACY \n",
    "#why are we splitting again here? shouldnt we fit on res anyways\n",
    "\n",
    "\n",
    "# Fit grid search\n",
    "best_model.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.best_params_, best_model.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final testing\n",
    "Pick best model and run on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @Neil - can you choose the best model and run it on the test data and paste the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination.\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Arenâ€™t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
